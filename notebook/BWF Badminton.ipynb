{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Data Analysis - Project\n",
    "___\n",
    "\n",
    "__Note!__ The submitted notebooks need to illustrate the knowledge of the Bayesian workflow\n",
    "\n",
    "__RUBRIC:__ \n",
    "* The introduction is inviting, presents an overview of the notebook. Information is relevant and presented in a logical order.\n",
    "* The conclusion is clear\n",
    "* The notebook presents a clear cohesive data analysis story, which is enjoyable to read\n",
    "* Accuracy of use of statistical terms: Statistical terms are used accurately and with clarity \n",
    "* Description of the data, and the analysis problem\n",
    "* Description of the model\n",
    "* Description of the prior choices: Priors are listed and justiï¬ed\n",
    "* Is Stan code included?\n",
    "* Is code for how Stan model is run included?\n",
    "*  Is required convergence diagnostics (Rhat, divergences, neff) included?: Required onvergence diagnostic results shown and maning of the results is discussed\n",
    "* Is there posterior predictive checking?\n",
    "*  Is there a discussion of problems and potential improvements ?\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "**Goal**: predict the outcome of a badminton match based on the ranking different between players\n",
    "\n",
    "**Approach**: apply Bayesian data analysis on historical data of badminton tournaments. The estimand of interest is the probability of winning with a certain ranking spread.\n",
    "\n",
    "**Implementation**:\n",
    "* Start with some naive assumption of the estimand, in order to choose the model later\n",
    "* Collect and preprocess data\n",
    "* Decide on prior choices and models\n",
    "* Do stan analysis on each models\n",
    "* Model comparision (using PSIS-LOO)\n",
    "* Do posterior predictive comparision between models\n",
    "* Conclusion, possible improvements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Analysis problem\n",
    "\n",
    "### 2.1 Discretizing the problem\n",
    "In one tournament, there are 8 seed players and some unranked players. To discretize the ranking spread, we chose 12th as the rank for all unranked players. The spread is calculated as follows:\n",
    "\n",
    "**Spread (from 1st player perspective) = Rank(2nd player) - Rank(1st player)**\n",
    "\n",
    "E.g.\n",
    "\n",
    "* Spread(from 1st rank player to 8th rank player) = 8 - 1 = 7\n",
    "* Spread(from 2nd rank player to unrank player) = 12 - 2 = 10\n",
    "* Spread(from unrank player to 3rd rank player) = 3 - 12 = -9\n",
    "\n",
    "The discrete space for ranking spread is then **[-11,-10,-9,...,9,10,11]**\n",
    "\n",
    "A match (which has at most 3 games) has 6 possible outcomes:\n",
    "1. Lose Lose     -> Lose\n",
    "2. Lose Win Lose -> Lose\n",
    "3. Win Lose Lose -> Lose\n",
    "4. Lose Win Win  -> Win\n",
    "5. Win Lose Win  -> Win\n",
    "6. Win Win       -> Win \n",
    "\n",
    "To discretize this parameter, we map the outcome of a match to **[1,2,3,4,5,6]** in terms of win degree (i.e. win degree 1 is the worst, and win degree 6 is the best)\n",
    "\n",
    "\n",
    "For the match below, ranking spread is 11, win degree is 4\n",
    "\n",
    "<img src=\"match_beautified.png\" alt=\"match\" style=\"width: 600px; margin-left: 0\"/>\n",
    "\n",
    "### 2.2 Modeling the problem\n",
    "\n",
    "Unless stated otherwise, all information in the data are from 1st player perspective\n",
    "An observation of a match includes 2 pieces of information:\n",
    "* ranking spread\n",
    "* win degree\n",
    "\n",
    "To formulate the observations as a one dimensional space collection, we need to reduce the matrix 23x6 (23 different spreads, 6 different win degrees) of all possible raw observations. The intuition of the reduction is as follows:\n",
    "* With the same ranking spread, higher win degree correlates to higher value (see arrow A in image below)\n",
    "* With the same win degree, lower ranking spread correlates to higher value (see arrow B in image below)\n",
    "* Step between value is 1\n",
    "\n",
    "With the given constraint, we will have **28 possible values of observation from [1,28]**. The mapping is as follow (columns are win degrees and rows are ranking spreads)\n",
    "\n",
    "<img src=\"mapping4.jpg\" alt=\"mapping\" style=\"width: 400px; margin-left: 0\"/>\n",
    "\n",
    "### 2.3 Analysing the problem\n",
    "\n",
    "The analysis problem is the distribution of observations and how they differ between different tournaments. Furthermore, we will also try and the analyze the affect of different models and priors choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dataset and data model\n",
    "\n",
    "The dataset is collected from Badminton World Federation (BWF) tournament database using Scrapy crawler. After collecting, the data is preprocessed as stated in the previous section. In the end, the format of data is similar to the factory data assignments. A peek of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament 1</th>\n",
       "      <th>Tournament 2</th>\n",
       "      <th>Tournament 3</th>\n",
       "      <th>Tournament 4</th>\n",
       "      <th>Tournament 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Match 1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 5</th>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tournament 1  Tournament 2  Tournament 3  Tournament 4  Tournament 5\n",
       "Match 1           6.0          17.0          16.0           6.0           1.0\n",
       "Match 2          15.0          15.0          17.0          13.0          17.0\n",
       "Match 3          16.0           7.0           4.0          17.0           5.0\n",
       "Match 4          15.0          13.0          17.0          17.0          13.0\n",
       "Match 5          19.0          20.0          20.0          15.0          17.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_first_rows_of_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament 1</th>\n",
       "      <th>Tournament 2</th>\n",
       "      <th>Tournament 3</th>\n",
       "      <th>Tournament 4</th>\n",
       "      <th>Tournament 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.776119</td>\n",
       "      <td>14.388060</td>\n",
       "      <td>13.746269</td>\n",
       "      <td>14.880597</td>\n",
       "      <td>14.164179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.746727</td>\n",
       "      <td>5.635266</td>\n",
       "      <td>5.329572</td>\n",
       "      <td>5.878885</td>\n",
       "      <td>5.703792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tournament 1  Tournament 2  Tournament 3  Tournament 4  Tournament 5\n",
       "count     67.000000     67.000000     67.000000     67.000000     67.000000\n",
       "mean      13.776119     14.388060     13.746269     14.880597     14.164179\n",
       "std        5.746727      5.635266      5.329572      5.878885      5.703792\n",
       "min        4.000000      3.000000      4.000000      3.000000      1.000000\n",
       "25%        9.000000     10.000000      8.000000     10.000000     10.000000\n",
       "50%       15.000000     15.000000     15.000000     16.000000     16.000000\n",
       "75%       17.000000     18.000000     17.000000     17.000000     17.000000\n",
       "max       25.000000     27.000000     26.000000     27.000000     27.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_summary_of_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prior choices\n",
    "\n",
    "We decided to use two different priors: \n",
    "* **Inverse gamma** is chosen on variance because it is the conjugate prior to normal likelihood and it has a closed form solution for the outcome of the posterior\n",
    "* **Uniform** is chosen as weak prior to observe how sensitive is outcome in regards the prior and the data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Model\n",
    "\n",
    "In normal distribution where $\\mu$ is known and $\\sigma^2$ is unknown, the marginal posterior distribution $p(\\sigma^2|y)$ can be computed as described below. The posterior distribution is computed using two different priors, whereas the first is an uniformative (uniform) and the second an informative (inverse gamma) prior.\n",
    "\n",
    "__Priors:__\n",
    "\n",
    "Uniform prior\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\sigma^2) \\propto Uniform(0, \\infty)\n",
    "\\end{equation*}\n",
    "\n",
    "Inverse gamma prior\n",
    "\n",
    "\\begin{equation*}  \n",
    "p(\\sigma^2) \\propto (\\sigma^2)^{-(a+1)}e^{-\\beta/\\sigma^2} \\propto Inv-Gamma(\\alpha, \\beta) \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are the shape and scale parameters. __The values of the parameters ??__\n",
    "\n",
    "__Likelihood:__\n",
    "\\begin{equation*}\n",
    "p(y|\\mu,\\sigma^2) \\propto \\prod_{i=1}^{N} p(y_i | \\mu, \\sigma^2) \\propto N(y | \\mu, \\sigma^2) \n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mu$ is known.\n",
    "\n",
    "__Posterior:__\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\sigma^2 | y) \\propto p(\\sigma^2)p(y|\\mu, \\sigma^2)\n",
    "\\end{equation*}\n",
    "\n",
    "Since one of the objective is to predict the distribution of a new tournament, we will use pooled and hierarchical model. The separate model is excluded because it handles the tournaments uniquely  without having any common parameters which could be used to predict the new tournament.\n",
    "\n",
    "In the pooled model the mean and the variance is computed from the combined data of all the tournaments and there is no distinction between different tournaments. This means that also the new tournament will have similar distibution as the predictive distribution of the tournaments.  \n",
    "\n",
    "In the hierarchical model each tournament is handled separately having own mean and common standard deviation. Furthermore, all the means are controlled by common hyperparameters ($\\mu_0$ and $\\sigma^2_0$) which means that the means are drawn from the common distribution described by these hyperparameters. The result of the new tournament can be predicted using the common hyperparameters: first draw the mean from the common distribution and use it to sample the predictive distribution. \n",
    "\n",
    "Then based on the prior choices, we have 4 different models:\n",
    "* pooled with uniform prior\n",
    "* pooled with inverse gamma prior for variance\n",
    "* hierarchical with uniform prior\n",
    "* hierarchical with inverse gamma prior for variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Stan analysis of the models\n",
    "\n",
    "For each model, we will show the Stan model and convergence diagnostic.\n",
    "\n",
    "The Stan model is fitted using Stan's default parameters (4 chains, 1000 warmup iterations, 1000 sampling iteration, ending up to 4000 samples and 10 as maximum tree depth). In addition, to avoid false positive conclusion about divergences, the *adapt_delta* value is set to 0.9. This means that the fitting uses larger target acceptance probability and therefore all the divergences can be seen. If the resulting value is still 0 after this, we can verify that there are no divergences. If not, the divergences could be furter analyzed by increasing *adapt_delta*.\n",
    "\n",
    "Besides divergences, the converge diagnostic includes a short discussion about $\\hat R$ and n_eff. Generally, if the $\\hat R$ values of the parameters are close to 1 and below 1.1, the fit has been good. The low $\\hat R$ values combined with high effective sample size (n_eff) per transition informs that the Markov chains were mixed well. Note that discussion about depth tree and energy Bayesian fraction of missing information (E-BFMI) is left out because their results were same for all the models (depth tree 0 and E-BFMI did not give any information).\n",
    "\n",
    "### 6.1 Pooled model with uniform prior\n",
    "\n",
    "#### Stan model\n",
    "\n",
    "The stan code of the model:\n",
    "```\n",
    "data {\n",
    "  int<lower=0> N;\t\t// Number of observations\n",
    "  vector[N] y;           // N observations for J tournaments\n",
    "}\n",
    "parameters {\n",
    "  real mu;              // Common mean\n",
    "  real<lower=0> sigma;  // Common std\n",
    "}\n",
    "model {\n",
    "  y ~ normal(mu, sigma);// Model for fitting data using tournament specific mu and common std\n",
    "}\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  real ypred;\n",
    "  \n",
    "  ypred = normal_rng(mu, sigma);\t              //Prediction of tournament\n",
    "  for (n in 1:N)\n",
    "    log_lik[n] = normal_lpdf(y[n] | mu, sigma);   //Log-likelihood\n",
    "}\n",
    "```\n",
    "#### Convergence diagnostic\n",
    "After compiling the model and fitting the combined data of the tournaments, the diagnostic of the fit was examined:\n",
    "\n",
    "| Diagnostic | Result   |\n",
    "|---|---|\n",
    "| All $\\hat R$ values close to 1 | OK |\n",
    "| Low $\\hat R$ values combined with high effective sample size (n_eff) | OK |\n",
    "| Divergences is 0 | OK |\n",
    "\n",
    "All the results for pooled uniform prior model fitting are good. The full fit can be found in the _Attachment 1_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>14.1935</td>\n",
       "      <td>0.00603298</td>\n",
       "      <td>0.314179</td>\n",
       "      <td>13.5613</td>\n",
       "      <td>13.9842</td>\n",
       "      <td>14.2001</td>\n",
       "      <td>14.409</td>\n",
       "      <td>14.8204</td>\n",
       "      <td>2712</td>\n",
       "      <td>1.00109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>5.66282</td>\n",
       "      <td>0.00441324</td>\n",
       "      <td>0.224035</td>\n",
       "      <td>5.24378</td>\n",
       "      <td>5.50642</td>\n",
       "      <td>5.65253</td>\n",
       "      <td>5.80839</td>\n",
       "      <td>6.12739</td>\n",
       "      <td>2577</td>\n",
       "      <td>1.00018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[0]</th>\n",
       "      <td>-3.70522</td>\n",
       "      <td>0.00177279</td>\n",
       "      <td>0.091105</td>\n",
       "      <td>-3.89356</td>\n",
       "      <td>-3.76527</td>\n",
       "      <td>-3.70164</td>\n",
       "      <td>-3.64204</td>\n",
       "      <td>-3.53086</td>\n",
       "      <td>2641</td>\n",
       "      <td>1.00078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[1]</th>\n",
       "      <td>-2.66381</td>\n",
       "      <td>0.000781838</td>\n",
       "      <td>0.0394576</td>\n",
       "      <td>-2.7439</td>\n",
       "      <td>-2.68967</td>\n",
       "      <td>-2.66273</td>\n",
       "      <td>-2.63632</td>\n",
       "      <td>-2.58826</td>\n",
       "      <td>2547</td>\n",
       "      <td>1.00023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[2]</th>\n",
       "      <td>-2.70475</td>\n",
       "      <td>0.000794411</td>\n",
       "      <td>0.0395853</td>\n",
       "      <td>-2.78604</td>\n",
       "      <td>-2.73049</td>\n",
       "      <td>-2.70434</td>\n",
       "      <td>-2.67761</td>\n",
       "      <td>-2.62968</td>\n",
       "      <td>2483</td>\n",
       "      <td>1.00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[332]</th>\n",
       "      <td>-2.70475</td>\n",
       "      <td>0.000794411</td>\n",
       "      <td>0.0395853</td>\n",
       "      <td>-2.78604</td>\n",
       "      <td>-2.73049</td>\n",
       "      <td>-2.70434</td>\n",
       "      <td>-2.67761</td>\n",
       "      <td>-2.62968</td>\n",
       "      <td>2483</td>\n",
       "      <td>1.00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[333]</th>\n",
       "      <td>-2.81336</td>\n",
       "      <td>0.000768998</td>\n",
       "      <td>0.0414903</td>\n",
       "      <td>-2.89455</td>\n",
       "      <td>-2.84142</td>\n",
       "      <td>-2.81289</td>\n",
       "      <td>-2.78428</td>\n",
       "      <td>-2.73293</td>\n",
       "      <td>2911</td>\n",
       "      <td>1.00091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[334]</th>\n",
       "      <td>-3.46419</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.0745643</td>\n",
       "      <td>-3.61728</td>\n",
       "      <td>-3.51306</td>\n",
       "      <td>-3.46244</td>\n",
       "      <td>-3.41316</td>\n",
       "      <td>-3.32091</td>\n",
       "      <td>2738</td>\n",
       "      <td>1.00092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred</th>\n",
       "      <td>14.2868</td>\n",
       "      <td>0.0901683</td>\n",
       "      <td>5.70274</td>\n",
       "      <td>3.42566</td>\n",
       "      <td>10.3648</td>\n",
       "      <td>14.2842</td>\n",
       "      <td>18.1957</td>\n",
       "      <td>25.5974</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-746.025</td>\n",
       "      <td>0.0248809</td>\n",
       "      <td>1.00668</td>\n",
       "      <td>-748.748</td>\n",
       "      <td>-746.425</td>\n",
       "      <td>-745.714</td>\n",
       "      <td>-745.295</td>\n",
       "      <td>-745.018</td>\n",
       "      <td>1637</td>\n",
       "      <td>0.999565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean      se_mean         sd     2.5%      25%      50%  \\\n",
       "mu            14.1935   0.00603298   0.314179  13.5613  13.9842  14.2001   \n",
       "sigma         5.66282   0.00441324   0.224035  5.24378  5.50642  5.65253   \n",
       "log_lik[0]   -3.70522   0.00177279   0.091105 -3.89356 -3.76527 -3.70164   \n",
       "log_lik[1]   -2.66381  0.000781838  0.0394576  -2.7439 -2.68967 -2.66273   \n",
       "log_lik[2]   -2.70475  0.000794411  0.0395853 -2.78604 -2.73049 -2.70434   \n",
       "...               ...          ...        ...      ...      ...      ...   \n",
       "log_lik[332] -2.70475  0.000794411  0.0395853 -2.78604 -2.73049 -2.70434   \n",
       "log_lik[333] -2.81336  0.000768998  0.0414903 -2.89455 -2.84142 -2.81289   \n",
       "log_lik[334] -3.46419     0.001425  0.0745643 -3.61728 -3.51306 -3.46244   \n",
       "ypred         14.2868    0.0901683    5.70274  3.42566  10.3648  14.2842   \n",
       "lp__         -746.025    0.0248809    1.00668 -748.748 -746.425 -745.714   \n",
       "\n",
       "                  75%    97.5% n_eff      Rhat  \n",
       "mu             14.409  14.8204  2712   1.00109  \n",
       "sigma         5.80839  6.12739  2577   1.00018  \n",
       "log_lik[0]   -3.64204 -3.53086  2641   1.00078  \n",
       "log_lik[1]   -2.63632 -2.58826  2547   1.00023  \n",
       "log_lik[2]   -2.67761 -2.62968  2483   1.00032  \n",
       "...               ...      ...   ...       ...  \n",
       "log_lik[332] -2.67761 -2.62968  2483   1.00032  \n",
       "log_lik[333] -2.78428 -2.73293  2911   1.00091  \n",
       "log_lik[334] -3.41316 -3.32091  2738   1.00092  \n",
       "ypred         18.1957  25.5974  4000  0.999995  \n",
       "lp__         -745.295 -745.018  1637  0.999565  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pooled uniform model\n",
    "pool_uni_df, pool_uni_fit = compute_model(r'stan_code/pool_uniform_prior.stan', pooled_data_model)\n",
    "# Print summary of the fit\n",
    "print_compact_fit(pool_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of the Rhat: \n",
      "1.0012218111524749\n",
      "\n",
      "Divergences:\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(pool_uni_fit, pool_uni_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 6.2 Pooled model with inverse gamma prior\n",
    "\n",
    "#### Stan model\n",
    "The stan code of the model:\n",
    "```\n",
    "data {\n",
    "  int<lower=0> N;\t\t // Number of observations\n",
    "  vector[N] y;            // N observations for J tournaments\n",
    "  real<lower=0.1> alpha;  //Shape\n",
    "  real<lower=0.1> beta;  //Scale\n",
    "}\n",
    "parameters {\n",
    "  real mu;               // Common mean\n",
    "  real<lower=0> sigmaSq; // Common var\n",
    "}\n",
    "transformed parameters {\n",
    "  real<lower=0> sigma;\n",
    "  sigma <- sqrt(sigmaSq);\n",
    "}\n",
    "model {\n",
    "  sigmaSq ~ inv_gamma(alpha,beta);  // Prior\n",
    "  y ~ normal(mu, sigma);            // Fitting of the model\n",
    "}\n",
    "generated quantities {\n",
    "  vector[N] log_lik;\n",
    "  real ypred;\n",
    "  \n",
    "  ypred = normal_rng(mu, sigma);\t// Prediction of tournament\n",
    "  for (n in 1:N)\n",
    "    log_lik[n] = normal_lpdf(y[n] | mu, sigma); //Log-likelihood\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic\n",
    "The same procedure is followed here (as in the previous section) and similar results were obtained:\n",
    "\n",
    "| Diagnostic | Result   |\n",
    "|---|---|\n",
    "| All $\\hat R$ values close to 1 | OK |\n",
    "| Low $\\hat R$ values combined with high effective sample size (n_eff) | OK |\n",
    "| Divergences is 0 | OK |\n",
    "\n",
    "All the results for pooled inverse gamma prior model fitting are good. The full fit can be found in the _Attachment 2_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>14.1912</td>\n",
       "      <td>0.00659562</td>\n",
       "      <td>0.311324</td>\n",
       "      <td>13.5848</td>\n",
       "      <td>13.9808</td>\n",
       "      <td>14.1944</td>\n",
       "      <td>14.3957</td>\n",
       "      <td>14.8102</td>\n",
       "      <td>2228</td>\n",
       "      <td>1.00237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigmaSq</th>\n",
       "      <td>31.819</td>\n",
       "      <td>0.0465451</td>\n",
       "      <td>2.42705</td>\n",
       "      <td>27.4953</td>\n",
       "      <td>30.0994</td>\n",
       "      <td>31.688</td>\n",
       "      <td>33.3725</td>\n",
       "      <td>36.8217</td>\n",
       "      <td>2719</td>\n",
       "      <td>1.00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>5.63676</td>\n",
       "      <td>0.0040921</td>\n",
       "      <td>0.21424</td>\n",
       "      <td>5.2436</td>\n",
       "      <td>5.48629</td>\n",
       "      <td>5.62921</td>\n",
       "      <td>5.77689</td>\n",
       "      <td>6.06809</td>\n",
       "      <td>2741</td>\n",
       "      <td>1.00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[0]</th>\n",
       "      <td>-3.70983</td>\n",
       "      <td>0.00189007</td>\n",
       "      <td>0.0933245</td>\n",
       "      <td>-3.89964</td>\n",
       "      <td>-3.77044</td>\n",
       "      <td>-3.7058</td>\n",
       "      <td>-3.64535</td>\n",
       "      <td>-3.53664</td>\n",
       "      <td>2438</td>\n",
       "      <td>1.00087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[1]</th>\n",
       "      <td>-2.65936</td>\n",
       "      <td>0.000735954</td>\n",
       "      <td>0.0384743</td>\n",
       "      <td>-2.73647</td>\n",
       "      <td>-2.68499</td>\n",
       "      <td>-2.65882</td>\n",
       "      <td>-2.63316</td>\n",
       "      <td>-2.58596</td>\n",
       "      <td>2733</td>\n",
       "      <td>0.999909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[332]</th>\n",
       "      <td>-2.70068</td>\n",
       "      <td>0.000759725</td>\n",
       "      <td>0.039352</td>\n",
       "      <td>-2.78049</td>\n",
       "      <td>-2.72698</td>\n",
       "      <td>-2.70021</td>\n",
       "      <td>-2.67379</td>\n",
       "      <td>-2.62523</td>\n",
       "      <td>2683</td>\n",
       "      <td>0.999668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[333]</th>\n",
       "      <td>-2.81015</td>\n",
       "      <td>0.000869736</td>\n",
       "      <td>0.0394174</td>\n",
       "      <td>-2.88917</td>\n",
       "      <td>-2.83559</td>\n",
       "      <td>-2.80985</td>\n",
       "      <td>-2.78294</td>\n",
       "      <td>-2.7354</td>\n",
       "      <td>2054</td>\n",
       "      <td>1.00321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[334]</th>\n",
       "      <td>-3.46668</td>\n",
       "      <td>0.0015697</td>\n",
       "      <td>0.0761101</td>\n",
       "      <td>-3.62117</td>\n",
       "      <td>-3.51465</td>\n",
       "      <td>-3.46372</td>\n",
       "      <td>-3.41446</td>\n",
       "      <td>-3.3255</td>\n",
       "      <td>2351</td>\n",
       "      <td>1.00139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred</th>\n",
       "      <td>14.2633</td>\n",
       "      <td>0.08992</td>\n",
       "      <td>5.61407</td>\n",
       "      <td>3.23815</td>\n",
       "      <td>10.4847</td>\n",
       "      <td>14.328</td>\n",
       "      <td>18.0029</td>\n",
       "      <td>25.224</td>\n",
       "      <td>3898</td>\n",
       "      <td>0.999202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-751.202</td>\n",
       "      <td>0.0262385</td>\n",
       "      <td>1.01486</td>\n",
       "      <td>-753.917</td>\n",
       "      <td>-751.573</td>\n",
       "      <td>-750.898</td>\n",
       "      <td>-750.491</td>\n",
       "      <td>-750.235</td>\n",
       "      <td>1496</td>\n",
       "      <td>1.00201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean      se_mean         sd     2.5%      25%      50%  \\\n",
       "mu            14.1912   0.00659562   0.311324  13.5848  13.9808  14.1944   \n",
       "sigmaSq        31.819    0.0465451    2.42705  27.4953  30.0994   31.688   \n",
       "sigma         5.63676    0.0040921    0.21424   5.2436  5.48629  5.62921   \n",
       "log_lik[0]   -3.70983   0.00189007  0.0933245 -3.89964 -3.77044  -3.7058   \n",
       "log_lik[1]   -2.65936  0.000735954  0.0384743 -2.73647 -2.68499 -2.65882   \n",
       "...               ...          ...        ...      ...      ...      ...   \n",
       "log_lik[332] -2.70068  0.000759725   0.039352 -2.78049 -2.72698 -2.70021   \n",
       "log_lik[333] -2.81015  0.000869736  0.0394174 -2.88917 -2.83559 -2.80985   \n",
       "log_lik[334] -3.46668    0.0015697  0.0761101 -3.62117 -3.51465 -3.46372   \n",
       "ypred         14.2633      0.08992    5.61407  3.23815  10.4847   14.328   \n",
       "lp__         -751.202    0.0262385    1.01486 -753.917 -751.573 -750.898   \n",
       "\n",
       "                  75%    97.5% n_eff      Rhat  \n",
       "mu            14.3957  14.8102  2228   1.00237  \n",
       "sigmaSq       33.3725  36.8217  2719   1.00037  \n",
       "sigma         5.77689  6.06809  2741   1.00037  \n",
       "log_lik[0]   -3.64535 -3.53664  2438   1.00087  \n",
       "log_lik[1]   -2.63316 -2.58596  2733  0.999909  \n",
       "...               ...      ...   ...       ...  \n",
       "log_lik[332] -2.67379 -2.62523  2683  0.999668  \n",
       "log_lik[333] -2.78294  -2.7354  2054   1.00321  \n",
       "log_lik[334] -3.41446  -3.3255  2351   1.00139  \n",
       "ypred         18.0029   25.224  3898  0.999202  \n",
       "lp__         -750.491 -750.235  1496   1.00201  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pooled inverse gamma model\n",
    "pool_inv_df, pool_inv_fit = compute_model(r'stan_code/pool_inverse_gamma_prior.stan', pooled_data_model)\n",
    "# Print summary of the fit\n",
    "print_compact_fit(pool_inv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of the Rhat: \n",
      "1.00330613987631\n",
      "\n",
      "Divergences:\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(pool_inv_fit, pool_inv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Hierarchical model with uniform prior\n",
    "\n",
    "### 6.3.1  Unsuccessful fitting\n",
    "\n",
    "#### Stan model \n",
    "The stan code of the model:\n",
    "```\n",
    "data {\n",
    "  int<lower=0> N;  // Number of observations\n",
    "  int<lower=0> J;  // Number of tournaments\n",
    "  matrix[N,J] y;  // N observations for J tournaments\n",
    "}\n",
    "parameters {\n",
    "  real mu0;             // Common mu for each J tournament's mu\n",
    "  real<lower=0> sigma0; // Common std for each J tournament's mu\n",
    "  real<lower=0> sigma; // Common std between tournaments\n",
    "  real mu_tilde[J];    // Tournament specific mu\n",
    "}\n",
    "model {\n",
    "  for (j in 1:J)\n",
    "    mu[j] ~ normal(mu0, sigma0);\t   // Model for computing tournament specific mu from common mu0 and sigma0\n",
    "  for (j in 1:J)\n",
    "    y[:,j] ~ normal(mu[j], sigma);\t// Model for fitting data using tournament specific mu and common std\n",
    "}\n",
    "generated quantities {\n",
    "  matrix[N,J] log_lik;    \n",
    "  real ypred[J];\n",
    "  real mu_new;\n",
    "  real ypred_new;\n",
    "  \n",
    "  for (j in 1:J)\n",
    "     ypred[j] = normal_rng(mu[j], sigma);\t// Predictive distibutions of all the tournaments\n",
    "  mu_new = normal_rng(mu0, sigma0);\t      // Next posterior distribution from commonly learned mu0 and sigma0\n",
    "  ypred_new = normal_rng(mu_new, sigma);\t// Next predictive distibutions of new tournament\n",
    "  \n",
    "  for (j in 1:J)\n",
    "     for (n in 1:N)\n",
    "        log_lik[n,j] = normal_lpdf(y[n,j] | mu[j], sigma); //Log-likelihood\n",
    "}\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic attempt 1\n",
    "The same procedure is followed here (as in the previous section) with minor change. The data set used for fitting is a matrix where columns are the tournaments and rows are the matches in the tournaments. The diagnostic results are:\n",
    "\n",
    "| Diagnostic | Result   |\n",
    "|--|--|\n",
    "| All $\\hat R$ values close to 1 | __NO__ <br> *lp__* is 1.4 |\n",
    "| Low $\\hat R$ values combined with high effective sample size (n_eff) | __NO__ |\n",
    "| Divergences is 0 | __NO__ <br> 8% of the target posterior was not explored|\n",
    "\n",
    "\n",
    "Because none of the conditions were fulfilled, in the next step we will try to improve the results by reducing the accuracy of the simulations by increasing the value of the *adapt_delta* parameter.\n",
    "\n",
    "The results of the attempt 1 can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>14.222</td>\n",
       "      <td>0.0306316</td>\n",
       "      <td>0.518028</td>\n",
       "      <td>13.2403</td>\n",
       "      <td>13.9695</td>\n",
       "      <td>14.264</td>\n",
       "      <td>14.4845</td>\n",
       "      <td>15.0738</td>\n",
       "      <td>286</td>\n",
       "      <td>1.0193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.560246</td>\n",
       "      <td>0.0731073</td>\n",
       "      <td>0.759754</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>0.172624</td>\n",
       "      <td>0.381537</td>\n",
       "      <td>0.709618</td>\n",
       "      <td>2.13192</td>\n",
       "      <td>108</td>\n",
       "      <td>1.03272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[0]</th>\n",
       "      <td>14.1029</td>\n",
       "      <td>0.0484593</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>13.0482</td>\n",
       "      <td>13.8036</td>\n",
       "      <td>14.1453</td>\n",
       "      <td>14.4149</td>\n",
       "      <td>14.938</td>\n",
       "      <td>97</td>\n",
       "      <td>1.03988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[1]</th>\n",
       "      <td>14.2979</td>\n",
       "      <td>0.0144251</td>\n",
       "      <td>0.458663</td>\n",
       "      <td>13.3891</td>\n",
       "      <td>14.0025</td>\n",
       "      <td>14.3344</td>\n",
       "      <td>14.5531</td>\n",
       "      <td>15.27</td>\n",
       "      <td>1011</td>\n",
       "      <td>1.0106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[2]</th>\n",
       "      <td>14.0964</td>\n",
       "      <td>0.0545487</td>\n",
       "      <td>0.496962</td>\n",
       "      <td>13.0192</td>\n",
       "      <td>13.7931</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>14.4328</td>\n",
       "      <td>14.9691</td>\n",
       "      <td>83</td>\n",
       "      <td>1.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>14.4326</td>\n",
       "      <td>0.0903994</td>\n",
       "      <td>5.6887</td>\n",
       "      <td>3.38663</td>\n",
       "      <td>10.585</td>\n",
       "      <td>14.451</td>\n",
       "      <td>18.2019</td>\n",
       "      <td>25.6342</td>\n",
       "      <td>3960</td>\n",
       "      <td>0.999456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>14.3471</td>\n",
       "      <td>0.0899178</td>\n",
       "      <td>5.6869</td>\n",
       "      <td>3.39243</td>\n",
       "      <td>10.5848</td>\n",
       "      <td>14.3392</td>\n",
       "      <td>18.2295</td>\n",
       "      <td>25.5183</td>\n",
       "      <td>4000</td>\n",
       "      <td>1.00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>14.208</td>\n",
       "      <td>0.0255003</td>\n",
       "      <td>1.1708</td>\n",
       "      <td>12.3481</td>\n",
       "      <td>13.8682</td>\n",
       "      <td>14.2873</td>\n",
       "      <td>14.5676</td>\n",
       "      <td>15.8755</td>\n",
       "      <td>2108</td>\n",
       "      <td>1.00535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>14.2926</td>\n",
       "      <td>0.0919394</td>\n",
       "      <td>5.81476</td>\n",
       "      <td>2.84883</td>\n",
       "      <td>10.4361</td>\n",
       "      <td>14.4064</td>\n",
       "      <td>18.2223</td>\n",
       "      <td>25.4268</td>\n",
       "      <td>4000</td>\n",
       "      <td>1.00122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-743.541</td>\n",
       "      <td>1.47587</td>\n",
       "      <td>4.89491</td>\n",
       "      <td>-752.243</td>\n",
       "      <td>-746.688</td>\n",
       "      <td>-744.196</td>\n",
       "      <td>-741.133</td>\n",
       "      <td>-732.58</td>\n",
       "      <td>11</td>\n",
       "      <td>1.42308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean    se_mean        sd      2.5%       25%       50%  \\\n",
       "mu0          14.222  0.0306316  0.518028   13.2403   13.9695    14.264   \n",
       "sigma0     0.560246  0.0731073  0.759754  0.034287  0.172624  0.381537   \n",
       "mu[0]       14.1029  0.0484593  0.477268   13.0482   13.8036   14.1453   \n",
       "mu[1]       14.2979  0.0144251  0.458663   13.3891   14.0025   14.3344   \n",
       "mu[2]       14.0964  0.0545487  0.496962   13.0192   13.7931   14.1295   \n",
       "...             ...        ...       ...       ...       ...       ...   \n",
       "ypred[3]    14.4326  0.0903994    5.6887   3.38663    10.585    14.451   \n",
       "ypred[4]    14.3471  0.0899178    5.6869   3.39243   10.5848   14.3392   \n",
       "mu_new       14.208  0.0255003    1.1708   12.3481   13.8682   14.2873   \n",
       "ypred_new   14.2926  0.0919394   5.81476   2.84883   10.4361   14.4064   \n",
       "lp__       -743.541    1.47587   4.89491  -752.243  -746.688  -744.196   \n",
       "\n",
       "                75%    97.5% n_eff      Rhat  \n",
       "mu0         14.4845  15.0738   286    1.0193  \n",
       "sigma0     0.709618  2.13192   108   1.03272  \n",
       "mu[0]       14.4149   14.938    97   1.03988  \n",
       "mu[1]       14.5531    15.27  1011    1.0106  \n",
       "mu[2]       14.4328  14.9691    83     1.044  \n",
       "...             ...      ...   ...       ...  \n",
       "ypred[3]    18.2019  25.6342  3960  0.999456  \n",
       "ypred[4]    18.2295  25.5183  4000   1.00017  \n",
       "mu_new      14.5676  15.8755  2108   1.00535  \n",
       "ypred_new   18.2223  25.4268  4000   1.00122  \n",
       "lp__       -741.133  -732.58    11   1.42308  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_uni_df, hier_uni_fit = compute_model(r'stan_code/hier_uniform_prior.stan', hierarchical_data_model)\n",
    "# Print the summary of the fit\n",
    "print_compact_fit(hier_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of the Rhat: \n",
      "1.423080255915649\n",
      "\n",
      "Divergences:\n",
      "308.0 of 4000 iterations ended with a divergence (7.7%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_uni_fit, hier_uni_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence diagnostic attempt 2\n",
    "\n",
    "After changing the accuracy of the simulations from 0.9 to 0.93, the data is re-fit and following results were gained:\n",
    "\n",
    "| Diagnostic | Result   |\n",
    "|--|--|\n",
    "| All $\\hat R$ values close to 1 | OK |\n",
    "| Low $\\hat R$ values combined with high effective sample size (n_eff) | OK |\n",
    "| Divergences is 0 | __NO__ <br> still 2% of the target posterior was not explored|\n",
    "\n",
    "With these results we can verify that the hierarchical uniform prior model fitting is almost successful. Although, this is not the desired result and therefore, in the next step the further improvement is discussed. \n",
    "\n",
    "The results of the attempt 2 is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>14.1722</td>\n",
       "      <td>0.0133645</td>\n",
       "      <td>0.44123</td>\n",
       "      <td>13.3144</td>\n",
       "      <td>13.9179</td>\n",
       "      <td>14.159</td>\n",
       "      <td>14.4377</td>\n",
       "      <td>15.0467</td>\n",
       "      <td>1090</td>\n",
       "      <td>1.00438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.563965</td>\n",
       "      <td>0.0243783</td>\n",
       "      <td>0.571721</td>\n",
       "      <td>0.0510735</td>\n",
       "      <td>0.200409</td>\n",
       "      <td>0.40928</td>\n",
       "      <td>0.731901</td>\n",
       "      <td>2.12706</td>\n",
       "      <td>550</td>\n",
       "      <td>1.00114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[0]</th>\n",
       "      <td>14.0559</td>\n",
       "      <td>0.0143581</td>\n",
       "      <td>0.480942</td>\n",
       "      <td>13.0217</td>\n",
       "      <td>13.7558</td>\n",
       "      <td>14.0839</td>\n",
       "      <td>14.3739</td>\n",
       "      <td>14.9692</td>\n",
       "      <td>1122</td>\n",
       "      <td>1.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[1]</th>\n",
       "      <td>14.2551</td>\n",
       "      <td>0.013469</td>\n",
       "      <td>0.481318</td>\n",
       "      <td>13.3542</td>\n",
       "      <td>13.9509</td>\n",
       "      <td>14.2275</td>\n",
       "      <td>14.5544</td>\n",
       "      <td>15.2806</td>\n",
       "      <td>1277</td>\n",
       "      <td>1.00411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[2]</th>\n",
       "      <td>14.0422</td>\n",
       "      <td>0.0138805</td>\n",
       "      <td>0.481636</td>\n",
       "      <td>12.9916</td>\n",
       "      <td>13.76</td>\n",
       "      <td>14.056</td>\n",
       "      <td>14.3534</td>\n",
       "      <td>14.9512</td>\n",
       "      <td>1204</td>\n",
       "      <td>1.00298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>14.3333</td>\n",
       "      <td>0.0915444</td>\n",
       "      <td>5.68976</td>\n",
       "      <td>3.33591</td>\n",
       "      <td>10.4447</td>\n",
       "      <td>14.3595</td>\n",
       "      <td>18.0902</td>\n",
       "      <td>25.5877</td>\n",
       "      <td>3863</td>\n",
       "      <td>0.99995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>14.2131</td>\n",
       "      <td>0.0884799</td>\n",
       "      <td>5.58756</td>\n",
       "      <td>3.2083</td>\n",
       "      <td>10.4591</td>\n",
       "      <td>14.2649</td>\n",
       "      <td>18.0418</td>\n",
       "      <td>24.8965</td>\n",
       "      <td>3988</td>\n",
       "      <td>1.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>14.1864</td>\n",
       "      <td>0.0184552</td>\n",
       "      <td>0.961623</td>\n",
       "      <td>12.4116</td>\n",
       "      <td>13.7922</td>\n",
       "      <td>14.1716</td>\n",
       "      <td>14.569</td>\n",
       "      <td>16.0339</td>\n",
       "      <td>2715</td>\n",
       "      <td>1.00083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>14.2098</td>\n",
       "      <td>0.0896986</td>\n",
       "      <td>5.67304</td>\n",
       "      <td>2.94677</td>\n",
       "      <td>10.4409</td>\n",
       "      <td>14.1165</td>\n",
       "      <td>17.8849</td>\n",
       "      <td>25.5395</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-744.229</td>\n",
       "      <td>0.245898</td>\n",
       "      <td>4.09994</td>\n",
       "      <td>-752.034</td>\n",
       "      <td>-747.03</td>\n",
       "      <td>-744.541</td>\n",
       "      <td>-741.593</td>\n",
       "      <td>-735.542</td>\n",
       "      <td>278</td>\n",
       "      <td>1.00407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean    se_mean        sd       2.5%       25%      50%  \\\n",
       "mu0         14.1722  0.0133645   0.44123    13.3144   13.9179   14.159   \n",
       "sigma0     0.563965  0.0243783  0.571721  0.0510735  0.200409  0.40928   \n",
       "mu[0]       14.0559  0.0143581  0.480942    13.0217   13.7558  14.0839   \n",
       "mu[1]       14.2551   0.013469  0.481318    13.3542   13.9509  14.2275   \n",
       "mu[2]       14.0422  0.0138805  0.481636    12.9916     13.76   14.056   \n",
       "...             ...        ...       ...        ...       ...      ...   \n",
       "ypred[3]    14.3333  0.0915444   5.68976    3.33591   10.4447  14.3595   \n",
       "ypred[4]    14.2131  0.0884799   5.58756     3.2083   10.4591  14.2649   \n",
       "mu_new      14.1864  0.0184552  0.961623    12.4116   13.7922  14.1716   \n",
       "ypred_new   14.2098  0.0896986   5.67304    2.94677   10.4409  14.1165   \n",
       "lp__       -744.229   0.245898   4.09994   -752.034   -747.03 -744.541   \n",
       "\n",
       "                75%    97.5% n_eff      Rhat  \n",
       "mu0         14.4377  15.0467  1090   1.00438  \n",
       "sigma0     0.731901  2.12706   550   1.00114  \n",
       "mu[0]       14.3739  14.9692  1122    1.0034  \n",
       "mu[1]       14.5544  15.2806  1277   1.00411  \n",
       "mu[2]       14.3534  14.9512  1204   1.00298  \n",
       "...             ...      ...   ...       ...  \n",
       "ypred[3]    18.0902  25.5877  3863   0.99995  \n",
       "ypred[4]    18.0418  24.8965  3988   1.00015  \n",
       "mu_new       14.569  16.0339  2715   1.00083  \n",
       "ypred_new   17.8849  25.5395  4000  0.999853  \n",
       "lp__       -741.593 -735.542   278   1.00407  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_uni_df, hier_uni_fit = compute_model(r'stan_code/hier_uniform_prior.stan', \n",
    "                                          hierarchical_data_model, adapt_delta=0.93)\n",
    "# Print the summary of the fit\n",
    "print_compact_fit(hier_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of the Rhat: \n",
      "1.0043815114485763\n",
      "\n",
      "Divergences:\n",
      "79.0 of 4000 iterations ended with a divergence (1.975%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_uni_fit, hier_uni_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Successful fitting\n",
    "\n",
    "In this section, we will modify the Stan code and use exactly the same approach as described in pystan's workflow (http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html, A Successful Fit). Because the fit of the centered parametrization is not successful, we should change the Stan code using non-centered parametrization.\n",
    "\n",
    "__Centered__ parametrization of parameter _mu_\n",
    "```\n",
    "parameters {\n",
    "  ...\n",
    "  real mu[J]; // Tournament specific mu\n",
    "  ...\n",
    "}\n",
    "model {\n",
    "  for (j in 1:J) // Model for computing tournament specific mu from common mu0 and sigma0\n",
    "    mu[j] ~ normal(mu0, sigma0);\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "is converted __to non-centered__ parametrization \n",
    "```\n",
    "parameters {\n",
    "  ...\n",
    "  real mu_tilde[J];\n",
    "}\n",
    "transformed parameters {\n",
    "  real mu[J];// Tournament specific mu\n",
    "  for (j in 1:J)\n",
    "    mu[j] = mu0 + sigma0 * mu_tilde[j];\n",
    "}\n",
    "model {\n",
    "  for (j in 1:J) // Model for computing tournament specific mu from common mu0 and sigma0\n",
    "    mu_tilde[j] ~ normal(0, 1); // Implies mu[j] ~ normal(mu0,sigma0)\n",
    "  ...\n",
    "}\n",
    "```\n",
    "The full updated Stan code is shown in the next section.\n",
    "\n",
    "#### Stan model \n",
    "The stan code of the model:\n",
    "```\n",
    "data {\n",
    "  int<lower=0> N;  // Number of observations\n",
    "  int<lower=0> J;  // Number of tournaments\n",
    "  matrix[N,J] y;  // N observations for J tournaments\n",
    "}\n",
    "parameters {\n",
    "  real mu0;             // Common mu for each J tournament's mu\n",
    "  real<lower=0> sigma0; // Common std for each J tournament's mu\n",
    "  real<lower=0> sigma; // Common std between tournaments\n",
    "    real mu_tilde[J];\n",
    "}\n",
    "transformed parameters {\n",
    "  real mu[J];          // Tournament specific mu\n",
    "  for (j in 1:J)\n",
    "    mu[j] = mu0 + sigma0 * mu_tilde[j];\n",
    "}\n",
    "model {\n",
    "  for (j in 1:J)                   // Model for computing tournament specific mu from common mu0 and sigma0\n",
    "    mu_tilde[j] ~ normal(0, 1);    // Implies mu[j] ~ normal(mu0,sigma0)\n",
    "  for (j in 1:J)\n",
    "    y[:,j] ~ normal(mu[j], sigma); // Model for fitting data using machine specific mu and common std\n",
    "}\n",
    "generated quantities {\n",
    "  matrix[N,J] log_lik;    \n",
    "  real ypred[J];\n",
    "  real mu_new;\n",
    "  real ypred_new;\n",
    "  \n",
    "  for (j in 1:J)\n",
    "     ypred[j] = normal_rng(mu[j], sigma);\t// Predictive distibutions of all the tournaments\n",
    "  mu_new = normal_rng(mu0, sigma0);\t      // Next posterior distribution from commonly learned mu0 and sigma0\n",
    "  ypred_new = normal_rng(mu_new, sigma);\t// Next predictive distibutions of new tournament\n",
    "  \n",
    "  for (j in 1:J)\n",
    "     for (n in 1:N)\n",
    "        log_lik[n,j] = normal_lpdf(y[n,j] | mu[j], sigma); //Log-likelihood\n",
    "}\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic attempt 3\n",
    "\n",
    "The same procedure is followed (as in the previous section) and improved results were obtained:\n",
    "\n",
    "| Diagnostic | Result   |\n",
    "|--|--|\n",
    "| All $\\hat R$ values close to 1 | OK |\n",
    "| Low $\\hat R$ values combined with high effective sample size (n_eff) | OK |\n",
    "| Divergences is 0 | GOOD ENOUGH <br> still 0.025% of the target posterior was not expolred <br> and increasing *adapt_delta* did not improve this result|\n",
    "\n",
    "All of these verify that hierarchical uniform prior model fitting is successful enough. The full fit can be found in the _Attachment 3_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>14.1961</td>\n",
       "      <td>0.0141698</td>\n",
       "      <td>0.458058</td>\n",
       "      <td>13.2531</td>\n",
       "      <td>13.9249</td>\n",
       "      <td>14.1917</td>\n",
       "      <td>14.464</td>\n",
       "      <td>15.1017</td>\n",
       "      <td>1045</td>\n",
       "      <td>1.00285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.550535</td>\n",
       "      <td>0.0179981</td>\n",
       "      <td>0.576781</td>\n",
       "      <td>0.0140076</td>\n",
       "      <td>0.18154</td>\n",
       "      <td>0.388406</td>\n",
       "      <td>0.717958</td>\n",
       "      <td>2.06309</td>\n",
       "      <td>1027</td>\n",
       "      <td>1.00309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>5.66836</td>\n",
       "      <td>0.00337786</td>\n",
       "      <td>0.213634</td>\n",
       "      <td>5.26576</td>\n",
       "      <td>5.52314</td>\n",
       "      <td>5.66113</td>\n",
       "      <td>5.80597</td>\n",
       "      <td>6.10786</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_tilde[0]</th>\n",
       "      <td>-0.207582</td>\n",
       "      <td>0.0159645</td>\n",
       "      <td>0.883688</td>\n",
       "      <td>-1.97272</td>\n",
       "      <td>-0.795004</td>\n",
       "      <td>-0.21698</td>\n",
       "      <td>0.373531</td>\n",
       "      <td>1.49234</td>\n",
       "      <td>3064</td>\n",
       "      <td>0.999875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_tilde[1]</th>\n",
       "      <td>0.10843</td>\n",
       "      <td>0.0154156</td>\n",
       "      <td>0.883542</td>\n",
       "      <td>-1.68338</td>\n",
       "      <td>-0.477808</td>\n",
       "      <td>0.101281</td>\n",
       "      <td>0.691408</td>\n",
       "      <td>1.87599</td>\n",
       "      <td>3285</td>\n",
       "      <td>1.00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>14.4115</td>\n",
       "      <td>0.0933838</td>\n",
       "      <td>5.77547</td>\n",
       "      <td>2.91519</td>\n",
       "      <td>10.6273</td>\n",
       "      <td>14.3439</td>\n",
       "      <td>18.2793</td>\n",
       "      <td>25.8232</td>\n",
       "      <td>3825</td>\n",
       "      <td>0.999828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>14.1985</td>\n",
       "      <td>0.0891236</td>\n",
       "      <td>5.63667</td>\n",
       "      <td>3.05899</td>\n",
       "      <td>10.3145</td>\n",
       "      <td>14.244</td>\n",
       "      <td>18.0424</td>\n",
       "      <td>24.8852</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>14.2178</td>\n",
       "      <td>0.0179958</td>\n",
       "      <td>0.940442</td>\n",
       "      <td>12.446</td>\n",
       "      <td>13.8592</td>\n",
       "      <td>14.2051</td>\n",
       "      <td>14.5741</td>\n",
       "      <td>16.0938</td>\n",
       "      <td>2731</td>\n",
       "      <td>0.999788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>14.198</td>\n",
       "      <td>0.0917411</td>\n",
       "      <td>5.80221</td>\n",
       "      <td>2.83785</td>\n",
       "      <td>10.2555</td>\n",
       "      <td>14.1872</td>\n",
       "      <td>18.1655</td>\n",
       "      <td>25.4919</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-749.31</td>\n",
       "      <td>0.0670804</td>\n",
       "      <td>2.27085</td>\n",
       "      <td>-754.317</td>\n",
       "      <td>-750.67</td>\n",
       "      <td>-749.121</td>\n",
       "      <td>-747.698</td>\n",
       "      <td>-745.474</td>\n",
       "      <td>1146</td>\n",
       "      <td>1.00138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean     se_mean        sd       2.5%       25%       50%  \\\n",
       "mu0           14.1961   0.0141698  0.458058    13.2531   13.9249   14.1917   \n",
       "sigma0       0.550535   0.0179981  0.576781  0.0140076   0.18154  0.388406   \n",
       "sigma         5.66836  0.00337786  0.213634    5.26576   5.52314   5.66113   \n",
       "mu_tilde[0] -0.207582   0.0159645  0.883688   -1.97272 -0.795004  -0.21698   \n",
       "mu_tilde[1]   0.10843   0.0154156  0.883542   -1.68338 -0.477808  0.101281   \n",
       "...               ...         ...       ...        ...       ...       ...   \n",
       "ypred[3]      14.4115   0.0933838   5.77547    2.91519   10.6273   14.3439   \n",
       "ypred[4]      14.1985   0.0891236   5.63667    3.05899   10.3145    14.244   \n",
       "mu_new        14.2178   0.0179958  0.940442     12.446   13.8592   14.2051   \n",
       "ypred_new      14.198   0.0917411   5.80221    2.83785   10.2555   14.1872   \n",
       "lp__          -749.31   0.0670804   2.27085   -754.317   -750.67  -749.121   \n",
       "\n",
       "                  75%    97.5% n_eff      Rhat  \n",
       "mu0            14.464  15.1017  1045   1.00285  \n",
       "sigma0       0.717958  2.06309  1027   1.00309  \n",
       "sigma         5.80597  6.10786  4000  0.999766  \n",
       "mu_tilde[0]  0.373531  1.49234  3064  0.999875  \n",
       "mu_tilde[1]  0.691408  1.87599  3285   1.00029  \n",
       "...               ...      ...   ...       ...  \n",
       "ypred[3]      18.2793  25.8232  3825  0.999828  \n",
       "ypred[4]      18.0424  24.8852  4000  0.999548  \n",
       "mu_new        14.5741  16.0938  2731  0.999788  \n",
       "ypred_new     18.1655  25.4919  4000  0.999715  \n",
       "lp__         -747.698 -745.474  1146   1.00138  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_uni_df, hier_uni_fit = compute_model(r'stan_code/hier_uniform_prior_v2.stan', hierarchical_data_model)\n",
    "# Print the summary of the fit\n",
    "print_compact_fit(hier_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of the Rhat: \n",
      "1.0030853274280642\n",
      "\n",
      "Divergences:\n",
      "1.0 of 4000 iterations ended with a divergence (0.025%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_uni_fit, hier_uni_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Hierarchical model with inverse gamma prior\n",
    "\n",
    "#### Stan model\n",
    "\n",
    "The stan code of the model (using non-centered parametrization):\n",
    "```\n",
    "data {\n",
    "  int<lower=0> N; \t\t\t// Number of observations\n",
    "  int<lower=0> J; \t\t\t// Number of tournaments\n",
    "  matrix[N,J] y; \t\t\t// N measurements for J tournaments\n",
    "}\n",
    "parameters {\n",
    "  real mu0;\t\t\t\t// Common mu for each J tournaments's mu\n",
    "  real<lower=0> sigma0;\t\t\t// Common std for each J tournament's mu\n",
    "  real<lower=0> sigma;\t\t\t// Common std\n",
    "  real mu_tilde[J];\n",
    "}\n",
    "transformed parameters {\n",
    "  real mu[J];\t\t\t// Tournament specific mu\n",
    "  for (j in 1:J)\n",
    "    mu[j] = mu0 + sigma0 * mu_tilde[j];\n",
    "}\n",
    "model {\n",
    "  for (j in 1:J) // Model for computing tournament specific mu from common mu0 and sigma0\n",
    "    mu_tilde[j] ~ normal(0, 1); // Implies mu[j] ~ normal(mu0,sigma0)\n",
    "  for (j in 1:J)\n",
    "    y[:,j] ~ normal(mu[j], sigma);\t// Model for fitting data using tournament specific mu and common std\n",
    "}\n",
    "generated quantities {\n",
    "  matrix[N,J] log_lik;    \n",
    "  real ypred[J];\n",
    "  real mu_new;\n",
    "  real ypred_new;\n",
    "  \n",
    "  for (j in 1:J)\n",
    "     ypred[j] = normal_rng(mu[j], sigma);\t// Predictive distibutions of all the tournaments\n",
    "  mu_new = normal_rng(mu0, sigma0);\t// Next posterior distribution from commonly learned mu0 and sigma0\n",
    "  ypred_new = normal_rng(mu_new, sigma);\t// Next predictive distibutions of new tournament\n",
    "  \n",
    "  for (j in 1:J)\n",
    "     for (n in 1:N)\n",
    "        log_lik[n,j] = normal_lpdf(y[n,j] | mu[j], sigma); //Log-likelihood\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic\n",
    "The same procedure is followed (as in the previous section) and the diagnostic results are:\n",
    "\n",
    "\n",
    "| Diagnostic | Result   |\n",
    "|--|--|\n",
    "| All $\\hat R$ values close to 1 | OK |\n",
    "| Low $\\hat R$ values combined with high effective sample size (n_eff) | OK |\n",
    "| Divergences is 0 | GOOD ENOUGH <br> still 0.025% of the target posterior was not expolred <br> and increasing *adapt_delta* did not improve this result|\n",
    "\n",
    "All of these verify that hierarchical inverse gamma prior model fitting is successful enough. The full fit can be found in the _Attachment 4_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>14.1742</td>\n",
       "      <td>0.0106244</td>\n",
       "      <td>0.429863</td>\n",
       "      <td>13.3323</td>\n",
       "      <td>13.9126</td>\n",
       "      <td>14.1783</td>\n",
       "      <td>14.4469</td>\n",
       "      <td>15.0025</td>\n",
       "      <td>1637</td>\n",
       "      <td>1.00147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.55408</td>\n",
       "      <td>0.0187816</td>\n",
       "      <td>0.567502</td>\n",
       "      <td>0.0227001</td>\n",
       "      <td>0.193471</td>\n",
       "      <td>0.403468</td>\n",
       "      <td>0.728336</td>\n",
       "      <td>2.04482</td>\n",
       "      <td>913</td>\n",
       "      <td>1.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_tilde[0]</th>\n",
       "      <td>-0.202636</td>\n",
       "      <td>0.0156438</td>\n",
       "      <td>0.883703</td>\n",
       "      <td>-1.96254</td>\n",
       "      <td>-0.760705</td>\n",
       "      <td>-0.211218</td>\n",
       "      <td>0.37144</td>\n",
       "      <td>1.55705</td>\n",
       "      <td>3191</td>\n",
       "      <td>1.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_tilde[1]</th>\n",
       "      <td>0.115359</td>\n",
       "      <td>0.0161837</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>-1.68287</td>\n",
       "      <td>-0.427827</td>\n",
       "      <td>0.10454</td>\n",
       "      <td>0.695337</td>\n",
       "      <td>1.82562</td>\n",
       "      <td>2959</td>\n",
       "      <td>1.00076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_tilde[2]</th>\n",
       "      <td>-0.200715</td>\n",
       "      <td>0.0146266</td>\n",
       "      <td>0.857001</td>\n",
       "      <td>-1.93516</td>\n",
       "      <td>-0.749326</td>\n",
       "      <td>-0.218871</td>\n",
       "      <td>0.354451</td>\n",
       "      <td>1.48706</td>\n",
       "      <td>3433</td>\n",
       "      <td>1.00081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>14.5288</td>\n",
       "      <td>0.0921506</td>\n",
       "      <td>5.67531</td>\n",
       "      <td>3.5257</td>\n",
       "      <td>10.7779</td>\n",
       "      <td>14.5297</td>\n",
       "      <td>18.2851</td>\n",
       "      <td>26.0277</td>\n",
       "      <td>3793</td>\n",
       "      <td>0.999918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>14.1416</td>\n",
       "      <td>0.0893992</td>\n",
       "      <td>5.6541</td>\n",
       "      <td>3.05447</td>\n",
       "      <td>10.3961</td>\n",
       "      <td>14.0928</td>\n",
       "      <td>17.9503</td>\n",
       "      <td>25.1413</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>14.1745</td>\n",
       "      <td>0.017315</td>\n",
       "      <td>0.895872</td>\n",
       "      <td>12.3202</td>\n",
       "      <td>13.8097</td>\n",
       "      <td>14.1975</td>\n",
       "      <td>14.5734</td>\n",
       "      <td>15.8751</td>\n",
       "      <td>2677</td>\n",
       "      <td>1.00043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>14.2066</td>\n",
       "      <td>0.0899841</td>\n",
       "      <td>5.69109</td>\n",
       "      <td>3.30853</td>\n",
       "      <td>10.3106</td>\n",
       "      <td>14.0458</td>\n",
       "      <td>18.1636</td>\n",
       "      <td>25.1357</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-754.52</td>\n",
       "      <td>0.0668638</td>\n",
       "      <td>2.37343</td>\n",
       "      <td>-759.858</td>\n",
       "      <td>-756.017</td>\n",
       "      <td>-754.246</td>\n",
       "      <td>-752.799</td>\n",
       "      <td>-750.641</td>\n",
       "      <td>1260</td>\n",
       "      <td>1.00191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean    se_mean        sd       2.5%       25%       50%  \\\n",
       "mu0           14.1742  0.0106244  0.429863    13.3323   13.9126   14.1783   \n",
       "sigma0        0.55408  0.0187816  0.567502  0.0227001  0.193471  0.403468   \n",
       "mu_tilde[0] -0.202636  0.0156438  0.883703   -1.96254 -0.760705 -0.211218   \n",
       "mu_tilde[1]  0.115359  0.0161837  0.880342   -1.68287 -0.427827   0.10454   \n",
       "mu_tilde[2] -0.200715  0.0146266  0.857001   -1.93516 -0.749326 -0.218871   \n",
       "...               ...        ...       ...        ...       ...       ...   \n",
       "ypred[3]      14.5288  0.0921506   5.67531     3.5257   10.7779   14.5297   \n",
       "ypred[4]      14.1416  0.0893992    5.6541    3.05447   10.3961   14.0928   \n",
       "mu_new        14.1745   0.017315  0.895872    12.3202   13.8097   14.1975   \n",
       "ypred_new     14.2066  0.0899841   5.69109    3.30853   10.3106   14.0458   \n",
       "lp__          -754.52  0.0668638   2.37343   -759.858  -756.017  -754.246   \n",
       "\n",
       "                  75%    97.5% n_eff      Rhat  \n",
       "mu0           14.4469  15.0025  1637   1.00147  \n",
       "sigma0       0.728336  2.04482   913    1.0023  \n",
       "mu_tilde[0]   0.37144  1.55705  3191    1.0007  \n",
       "mu_tilde[1]  0.695337  1.82562  2959   1.00076  \n",
       "mu_tilde[2]  0.354451  1.48706  3433   1.00081  \n",
       "...               ...      ...   ...       ...  \n",
       "ypred[3]      18.2851  26.0277  3793  0.999918  \n",
       "ypred[4]      17.9503  25.1413  4000  0.999683  \n",
       "mu_new        14.5734  15.8751  2677   1.00043  \n",
       "ypred_new     18.1636  25.1357  4000  0.999641  \n",
       "lp__         -752.799 -750.641  1260   1.00191  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_inv_df, hier_inv_fit = compute_model(r'stan_code/hier_inverse_gamma_prior_v2.stan', hierarchical_data_model)\n",
    "print_compact_fit(hier_inv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of the Rhat: \n",
      "1.0022963849717355\n",
      "\n",
      "Divergences:\n",
      "1.0 of 4000 iterations ended with a divergence (0.025%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_inv_fit, hier_inv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Conclusion\n",
    "\n",
    "Based on the diagnostic results, all the four models can be used for further analysis (next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Model comparision with PSIS-LOO and $P_{LOO-CV}$ \n",
    "\n",
    "\n",
    "* Model selection according to the hightest LOO-CV sum\n",
    "* Reliability based on the _k_values: <0.7 ok, <0.5 good\n",
    "\n",
    "\n",
    "\n",
    "__TO DO!__ The PSIS-LOO values of the models can be computed using provided _psisloo_ function. The function returns observation specific _k_-values and PSIS-LOO-CV values. In addition it return the sum of the PSIS-LOO-CV values, hence the sum of the LOO log desnities:\n",
    "\n",
    "\\begin{equation*}\n",
    "lppd_{loo-cv} = \\sum_{i=1}^{N} log \\left( \\frac{1}{S} \\sum_{s=1}^{S} p(y_i|\\theta^{is}) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "The estimated effective number of parameters ($P_{LOO-CV}$) in the model is computed as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "p_{loo-cv} = lppd-lppd_{loo-cv} \n",
    "\\end{equation*}\n",
    "\n",
    "where $lppd$ is the sum of the log densities of the posterior draws:\n",
    "\\begin{equation*}\n",
    "lppd = \\sum_{i=1}^{N} log \\left( \\frac{1}{S} \\sum_{s=1}^{S} p(y_i|\\theta^{s}) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "All the PSIS-LOO values, estimated effective number of parameters and plotted _k_-values are shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Posterior predictive checking \n",
    "\n",
    "* show predictive distributions\n",
    "* show the observations of tournaments and their predictions\n",
    "* show the obesrvations of new tournment and their predictions\n",
    "\n",
    "### 8.1 Pooled uniform\n",
    "\n",
    "### 8.2 Pooled inverse gamma\n",
    "\n",
    "### 8.3 Hierarchical uniform\n",
    "\n",
    "### 8.4 Hierarchical inverse gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Note that for hierarchical models, the new predictive tournament is computed from common hyperparatemers\n",
    "# ypreds = [\n",
    "#     pool_uni_fit.extract(permuted=True)['ypred'],     #Pooled unifrom new predictive tournament\n",
    "#     pool_inv_fit.extract(permuted=True)['ypred'],     #Pooled inverse gamma new predictive tournament\n",
    "#     hier_uni_fit.extract(permuted=True)['ypred_new'], #Hierarchical unifrom new predictive tournament \n",
    "#     hier_inv_fit.extract(permuted=True)['ypred_new']  #Hierarchical inverse gamma new predictive tournament\n",
    "# ]\n",
    "# ypreds_labels = ['Pooled uniform', 'Pooled inverse gamma', \"Hierarchical uniform\", \"Hierarchical inverse gamma\"]\n",
    "\n",
    "#print(\"The predictive distributions of the models and the actual distribution\")\n",
    "#compare_new_predictive_against_actual_distribution(ypreds,pooled_data,ypreds_labels,'Original data set')\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "ypred = hier_inv_fit.extract(permuted=True)['ypred_new']\n",
    "test_data = last\n",
    "x = np.linspace(0,50, 50)\n",
    "y = stats.norm(np.mean(test_data), np.std(test_data)).pdf(x)\n",
    "plt.hist(test_data, bins=20, normed=True, alpha=0.3, label=\"6th tournament\", color=\"C0\")\n",
    "plt.plot(x,y, label=\"6th tournament normal pdf\", color=\"C0\")\n",
    "\n",
    "y2 = stats.norm(np.mean(ypred), np.std(ypred)).pdf(x)\n",
    "plt.hist(ypred, bins=20, normed=True, alpha=0.3, label=\"ypred new samples\", color=\"C1\")\n",
    "plt.plot(x,y2,label=\"ypred new normal pdf\", color=\"C1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ypred = hier_uni_fit.extract(permuted=True)['ypred_new']\n",
    "test_data = last\n",
    "x = np.linspace(0,50, 50)\n",
    "y = stats.norm(np.mean(test_data), np.std(test_data)).pdf(x)\n",
    "plt.hist(test_data, bins=20, normed=True, alpha=0.3, label=\"6th tournament\", color=\"C0\")\n",
    "plt.plot(x,y, label=\"6th tournament normal pdf\", color=\"C0\")\n",
    "\n",
    "y2 = stats.norm(np.mean(ypred), np.std(ypred)).pdf(x)\n",
    "plt.hist(ypred, bins=20, normed=True, alpha=0.3, label=\"ypred new samples\", color=\"C1\")\n",
    "plt.plot(x,y2,label=\"ypred new normal pdf\", color=\"C1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ypred = pool_uni_fit.extract(permuted=True)['ypred']\n",
    "test_data = last\n",
    "x = np.linspace(0,50, 50)\n",
    "y = stats.norm(np.mean(test_data), np.std(test_data)).pdf(x)\n",
    "plt.hist(test_data, bins=20, normed=True, alpha=0.3, label=\"6th tournament\", color=\"C0\")\n",
    "plt.plot(x,y, label=\"6th tournament normal pdf\", color=\"C0\")\n",
    "\n",
    "y2 = stats.norm(np.mean(ypred), np.std(ypred)).pdf(x)\n",
    "plt.hist(ypred, bins=20, normed=True, alpha=0.3, label=\"ypred new samples\", color=\"C1\")\n",
    "plt.plot(x,y2,label=\"ypred new normal pdf\", color=\"C1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ypred = pool_inv_fit.extract(permuted=True)['ypred']\n",
    "test_data = last\n",
    "x = np.linspace(0,50, 50)\n",
    "y = stats.norm(np.mean(test_data), np.std(test_data)).pdf(x)\n",
    "plt.hist(test_data, bins=20, normed=True, alpha=0.3, label=\"6th tournament\", color=\"C0\")\n",
    "plt.plot(x,y, label=\"6th tournament normal pdf\", color=\"C0\")\n",
    "\n",
    "y2 = stats.norm(np.mean(ypred), np.std(ypred)).pdf(x)\n",
    "plt.hist(ypred, bins=20, normed=True, alpha=0.3, label=\"ypred new samples\", color=\"C1\")\n",
    "plt.plot(x,y2,label=\"ypred new normal pdf\", color=\"C1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# fig, axes = plt.subplots(1, 1, sharey=True,figsize=(16,4), subplot_kw=dict(aspect='auto'))\n",
    "# fig.suptitle('Predictive distributions of the blablabla')\n",
    "\n",
    "# axes.scatter([i for i in range(len(pooled_data))],pooled_data,marker=\"o\", color=\"C1\", alpha=0.5)\n",
    "# axes.scatter([i for i in range(len(pooled_data))],pooled_data,marker=\"x\", color=\"C0\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# the predictive distribution of all the models look similar, \n",
    "# therefore the prediction of the new tournament looks similar as the one which was left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CODE EXAMPLE FOR PLOTTING SEVERAL AXES, REMOVE IT IF NOT NEEDED!!!\"\"\"\n",
    "\n",
    "#Show predictive distributions for all the machines and for new machine\n",
    "def show_predictive_distributions(samples):\n",
    "    m = 0;\n",
    "   \n",
    "    #First row of plots: machines from 1 to 4\n",
    "    fig, axes = plt.subplots(1, 4, sharey=True,figsize=(16,4), subplot_kw=dict(aspect='auto'))\n",
    "    fig.suptitle('Predictive distributions of the machines')\n",
    "    for i in range(0,4):\n",
    "        axes[i].set_title('Machine '+str(m+1))\n",
    "        mu = np.mean(samples[\"ypred\"][:,m])\n",
    "        if mu<85:\n",
    "            axes[i].hist(samples[\"ypred\"][:,m],50, density=True, alpha=0.3, color='R')\n",
    "        else:\n",
    "            axes[i].hist(samples[\"ypred\"][:,m],50, density=True, alpha=0.7)\n",
    "        axes[i].plot([85,85],[0.027, 0],'-',color='K',linewidth=1.5, label=r'$\\theta=85$')\n",
    "        axes[i].plot([mu,mu],[0.027, 0],'--',color='K', linewidth=1.5, label=r'$\\theta$ mean')\n",
    "        m+=1\n",
    "        axes[i].legend()\n",
    "    plt.show()   \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9 Sensitivity analysis (?)\n",
    "* on the priors \n",
    "* and the model (ask ta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Conclusion\n",
    "* problems: \n",
    " - data model is not 100& justifiable \n",
    "* potential improvements\n",
    " - data model can be modified so that multinomial can be used for likelihood\n",
    " - extensive experiment with binomial model\n",
    "* discussion\n",
    " - it's accurate\n",
    "* conclusion of the data analysis\n",
    "* Is there a discussion of problems and potential improvements ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pystan\n",
    "import stan_utility\n",
    "import psis\n",
    "import plot_tools\n",
    "\n",
    "# For hiding warnings that do not effect the functionality of the code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is provided as files, 1 file contains all matches result of a tournament\n",
    "# We will use data from all tournaments to fit model\n",
    "# Except for the last one which will be used to evaluate prediction accuracy\n",
    "data = []\n",
    "filenames = os.listdir(r'./data/')\n",
    "for idx, filename in enumerate(filenames):\n",
    "    col = np.loadtxt(f'data//{filename}').tolist()[0:67]\n",
    "    if (idx == (len(filenames) - 1)):\n",
    "        last = col[0:67]\n",
    "    else:\n",
    "        data.append(col)\n",
    "\n",
    "np_data = np.array(data)\n",
    "\n",
    "def show_first_rows_of_data():\n",
    "    df = pd.DataFrame(np_data.T)\n",
    "    df.columns=['Tournament '+str(i+1) for i in range(np_data.shape[0])]\n",
    "    df = df.rename({i: 'Match '+str(i+1) for i in range(np_data.shape[1])}, axis='index')\n",
    "    return df.head()\n",
    "\n",
    "def show_summary_of_data():\n",
    "    df = pd.DataFrame(np_data.T)\n",
    "    df.columns=['Tournament '+str(i+1) for i in range(np_data.shape[0])]\n",
    "    return df.describe()\n",
    "\n",
    "# print number of observations (matches) and number of predictors (tournaments)\n",
    "\n",
    "def print_model(file_path):\n",
    "    with open(file_path) as file:\n",
    "        print(file.read())\n",
    "\n",
    "pooled_data = np_data.flatten()\n",
    "pooled_data_model = dict(N=len(pooled_data), y=pooled_data, alpha=1, beta=1)\n",
    "\n",
    "pooled_inv_g_data_model = dict(N=len(pooled_data), y=pooled_data)\n",
    "\n",
    "hierarchical_data_model = dict(N = np_data.shape[1], J= np_data.shape[0],y = np_data.T, alpha=1, beta=1)\n",
    "\n",
    "'''\n",
    "adapt_delta: 0...1\n",
    "    Effects to divergences, hence to the accuracy of the posterior. \n",
    "    The smaller the value is the more strict the Stan model is in accepting sampels.\n",
    "    The bigger the value is the easier the Stan model accepts samples. \n",
    "'''\n",
    "def compute_model(file_path, data, chains=4, iter=2000, adapt_delta=0.9):\n",
    "    # Compile model for both separated and pooled\n",
    "    model = stan_utility.compile_model(file_path) \n",
    "\n",
    "    # Fit model: adapt_delta is used for divergences\n",
    "    fit = model.sampling(data=data, seed=194838,chains=chains, iter=iter, control=dict(adapt_delta=adapt_delta))\n",
    "\n",
    "    # get summary of the fit, use pandas data frame for layout\n",
    "    summary = fit.summary()\n",
    "    df = pd.DataFrame(summary['summary'], index=summary['summary_rownames'], columns=summary['summary_colnames'])\n",
    "    \n",
    "    return df, fit\n",
    "\n",
    "def print_compact_fit(fit_df, number_of_rows_head=5, number_of_rows_tail=5):\n",
    "    df = fit_df.head(number_of_rows_head)\n",
    "    df = df.append([{'mean':'...','se_mean':'...','sd':'...','2.5%':'...','25%':'...',\n",
    "                   '50%':'...','75%':'...','97.5%':'...','n_eff':'...','Rhat':'...'}])\n",
    "    df = df.rename({0: '...'}, axis='index')\n",
    "    df = df.append(fit_df.tail(number_of_rows_tail))\n",
    "    return df\n",
    "\n",
    "def print_compact_fit_checking(fit, df):\n",
    "    #Check the maximum value of the Rhat\n",
    "    print(\"Maximum value of the Rhat: \")\n",
    "    print(df.describe()['Rhat'][7])\n",
    "    print(\"\")\n",
    "    \n",
    "    # Check divergences\n",
    "    print(\"Divergences:\")\n",
    "    stan_utility.check_div(fit)\n",
    "    print(\"\")\n",
    "\n",
    "def compare_new_predictive_against_actual_distribution(new_preds, actual, new_pred_labels, actual_label):\n",
    "    fig, axes = plot_tools.hist_multi_sharex(\n",
    "        [new_preds[i] for i in range(len(new_preds))]+[actual],\n",
    "        rowlabels= [ new_pred_labels[i] for i in range(len(new_pred_labels))]+[actual_label],\n",
    "        n_bins=30,x_lines=np.mean(actual),figsize=(7, 10) )\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def compare_psis_loo(log_liks):\n",
    "    ## compute psis_loo, p_eff\n",
    "    ## plot k-values\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 1: Fit of pooled model with uniform prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pool_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 2: Fit of pooled model with inverse gamma prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pool_inv_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 3: Fit of hierarchical model with uniform prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hier_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 4: Fit of hierarchical model with inverse gamma prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hier_inv_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "REMOVE EVERYTHING STARTING HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt('data.txt')\n",
    "\n",
    "# Aggregate raw result\n",
    "aggregated_result = {}\n",
    "for i in range(-11,12):\n",
    "    aggregated_result[str(i)] = []\n",
    "\n",
    "for (spread, result) in raw_data:\n",
    "    key = str(int(spread))\n",
    "    aggregated_result[key].append(result)\n",
    "        \n",
    "# Turn to 23 x 3 matrix, where columns are win, lose, number of games, and rows are the possible spread\n",
    "data_matrix = []\n",
    "for key, value in aggregated_result.items():\n",
    "    total = len(value)\n",
    "    win = len([i for i in value if i > 0])\n",
    "    data_matrix.append([win, total - win, total])\n",
    "\n",
    "print(data_matrix)\n",
    "print(len(data_matrix))\n",
    "print(aggregated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read data and convert it to matrix with dimensions [6x23]\n",
    "raw_data = pd.read_csv('data.txt', sep=\" \", header=None)\n",
    "raw_data.columns = ['spread','win']\n",
    "raw_data = raw_data.groupby(['spread', 'win']).size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "data_json = {i: [0 for j in range(-11,12)] for i in range(1,7)}\n",
    "for index, row in raw_data.iterrows():\n",
    "    data_json[row['win']][11+row['spread']] = row['counts']\n",
    "\n",
    "data = pd.DataFrame(data=data_json)\n",
    "def getRowText():\n",
    "    j=-11\n",
    "    c = {}\n",
    "    for i in range(0,23):\n",
    "        c[i] = j\n",
    "        j += 1\n",
    "    return c\n",
    "\n",
    "data.rename(index=getRowText(), inplace=True)\n",
    "data = data.T\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertData(dataset):\n",
    "    win = dataset[0:3].sum(axis=0).values \n",
    "    lose = dataset[3:7].sum(axis=0).values \n",
    "    return np.array([win,lose])\n",
    "\n",
    "tt = convertData(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data #######\n",
    "raw_data = pd.read_csv('kristel.txt', sep=\" \", header=None)\n",
    "print(raw_data)\n",
    "\n",
    "\n",
    "highest_frequency = max([8,8,7,7,6,6,5,5,4,3,2])+2\n",
    "#data = dict(N=11, n=[highest_frequency for i in range(0,11)], \n",
    "#            y=np.array(list(reversed([8,8,7,7,6,6,5,5,4,3,2]))), x=[1,2,3,4,5,6,7,8,9,10,11])\n",
    "data = dict(r=23, c=3, y=data_matrix)\n",
    "\n",
    "def fit_model(model_code='stan_code\\\\binom-logistic-regression.stan'):\n",
    "    model = stan_utility.compile_model(model_code)\n",
    "    fit = model.sampling(data=data, seed=194838, chains=4, iter=4000)\n",
    "    samples = fit.extract(permuted=True)\n",
    "    print(fit)\n",
    "    stan_utility.check_treedepth(fit)\n",
    "    stan_utility.check_energy(fit)\n",
    "    stan_utility.check_div(fit)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "with open('stan_code\\\\binom-logistic-regression.stan') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# logistic_samples = fit_model('stan_code\\\\binom-logistic-regression.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_samples = fit_model('stan_code\\\\multinomial.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Multinomial #######\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_fit(x, y, mu):    \n",
    "    figsize = plt.rcParams['figure.figsize'].copy()\n",
    "    figsize[0] *= 2  # width\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # scatterplot and lines\n",
    "    color_scatter = 'C0'  # 'C0' for default color #0\n",
    "    color_line = 'C1'     # 'C1' for default color #1\n",
    "    # lighten color_line\n",
    "    color_shade = (1 - 0.1*(1 - np.array(mpl.colors.to_rgb(color_line))))\n",
    "    # plot\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        np.percentile(mu, 5, axis=0),\n",
    "        np.percentile(mu, 95, axis=0),\n",
    "        color=color_shade\n",
    "    )\n",
    "    ax.plot(\n",
    "        x,\n",
    "        np.percentile(mu, 50, axis=0),\n",
    "        color=color_line,\n",
    "        linewidth=1\n",
    "    )\n",
    "    ax.scatter(x, y, 5, color=color_scatter)\n",
    "    ax.set_xlabel('Spread')\n",
    "    ax.set_ylabel('Normalized frequency')\n",
    "    ax.set_title('Wins per spread')\n",
    "    plt.show()\n",
    "    \n",
    "show_model_fit(data[\"x\"],data[\"y\"]/np.array([highest_frequency]), logistic_samples['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"from scipy import stats\n",
    "for i in range(0,11):\n",
    "    mu = np.mean(logistic_samples['y_rep'][:,i])\n",
    "    std = np.std(logistic_samples['y_rep'][:,i])\n",
    "    x = np.linspace(0,11,100)\n",
    "    plt.plot(x, stats.norm(mu,std).pdf(x))\n",
    "    #plt.hist(logistic_samples['y_rep'][:,i], alpha=0.2, density=True)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plt.scatter(logistic_samples[\"alpha\"],logistic_samples[\"beta\"])\n",
    "plt.show();\n",
    "plt.hist(logistic_samples[\"alpha\"], bins=50)\n",
    "plt.hist(logistic_samples[\"beta\"], bins=50)\n",
    "plt.show();\n",
    "\n",
    "\"\"\"plt.scatter(data[\"x\"],data[\"y\"]/np.array([highest_frequency]))\n",
    "def calc(a,b,xp):\n",
    "    return np.exp(a + b*xp)/(1 + np.exp(a + b*xp))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR PSISLOO IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPPD: Posterior predictive distribution summarized by the simulation draws of theta^s\n",
    "def compute_LPPD(samples):\n",
    "    lppd_all = 0\n",
    "    samples = np.array(samples)\n",
    "    N = samples.shape[1]\n",
    "    S = samples.shape[0]\n",
    "    for i in range(0,N):\n",
    "        lppd_all += np.log(np.sum(np.exp(samples[:,i]))/S)\n",
    "    return lppd_all\n",
    "\n",
    "# LPPD LOO-CV: Bayesian loo-cv (leave-one-out cross-validation) estimate of out-of-sample predictive fit\n",
    "def compute_PSIS_LOO_values_and_plot_k(samples, model_text='Separate model'):\n",
    "    # Compute bayesian loo-cv using psisloo function\n",
    "    lppd_loo_cv, lppd_loos_cv, lppd_loo_k = psis.psisloo(samples)\n",
    "\n",
    "    #Estimate of the effective number of parameters\n",
    "    p_loo_cv = compute_LPPD(samples) - lppd_loo_cv\n",
    "    \n",
    "    # Plot k-values\n",
    "    plt.scatter(range(0,samples.shape[1]),lppd_loo_k,label=model_text, alpha=0.7)\n",
    "    \n",
    "    return {'loo_cv':lppd_loo_cv, 'loos_cv':lppd_loos_cv, 'loo_k':lppd_loo_k, 'p_loo_cv': p_loo_cv}\n",
    "\n",
    "#Reshape matrix of [SxNxJ] to [Sx(N*J)]\n",
    "def reshape_array(samples):\n",
    "    samples_all = np.array(samples)\n",
    "    S = samples_all.shape[0]\n",
    "    N = samples_all.shape[1]\n",
    "    J = samples_all.shape[2]\n",
    "    samples_reshaped = []\n",
    "    for s in range(0,S):\n",
    "        temp = []\n",
    "        for n in range(0,N):\n",
    "            for j in range(0,J):\n",
    "                temp.append(samples_all[s,n,j])\n",
    "        samples_reshaped.append(temp)\n",
    "    return np.array(samples_reshaped)\n",
    "\n",
    "\n",
    "#samples = sfit.extract(permuted=True)\n",
    "#compute_PSIS_LOO_values_and_plot_k(reshape_array(samples[\"log_lik\"]))\n",
    "\n",
    "# check the number of large (> 0.5) Pareto k estimates\n",
    "#np.sum(ks1 > 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
