{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Data Analysis - Project\n",
    "___\n",
    "\n",
    "__Note!__ The submitted notebooks need to illustrate the knowledge of the Bayesian workflow\n",
    "\n",
    "__RUBRIC:__ \n",
    "* The introduction is inviting, presents an overview of the notebook. Information is relevant and presented in a logical order.\n",
    "* The conclusion is clear\n",
    "* The notebook presents a clear cohesive data analysis story, which is enjoyable to read\n",
    "* Accuracy of use of statistical terms: Statistical terms are used accurately and with clarity \n",
    "* Description of the data, and the analysis problem\n",
    "* Description of the model\n",
    "* Description of the prior choices: Priors are listed and justiï¬ed\n",
    "* Is Stan code included?\n",
    "* Is code for how Stan model is run included?\n",
    "*  Is required convergence diagnostics (Rhat, divergences, neff) included?: Required onvergence diagnostic results shown and maning of the results is discussed\n",
    "* Is there posterior predictive checking?\n",
    "*  Is there a discussion of problems and potential improvements ?\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "**Goal**: predict the outcome of a badminton match based on the ranking different between players\n",
    "\n",
    "**Approach**: apply Bayesian data analysis on historical data of badminton tournaments. The estimand of interest is the probability of winning with a certain ranking spread.\n",
    "\n",
    "**Implementation**:\n",
    "* Start with some naive assumption of the estimand, in order to choose the model later\n",
    "* Collect and preprocess data\n",
    "* Decide on prior choices and models\n",
    "* Do stan analysis on each models\n",
    "* Model comparision (using PSIS-LOO)\n",
    "* Do posterior predictive comparision between models\n",
    "* Conclusion, possible improvements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Analysis problem\n",
    "\n",
    "### 2.1 Discretizing the problem\n",
    "In one tournament, there are 8 seed players and some unranked players. To discretize the ranking spread, we chose 12th as the rank for all unranked players. The spread is calculated as follows:\n",
    "\n",
    "**Spread (from 1st player perspective) = Rank(2nd player) - Rank(1st player)**\n",
    "\n",
    "E.g.\n",
    "\n",
    "* Spread(from 1st rank player to 8th rank player) = 8 - 1 = 7\n",
    "* Spread(from 2nd rank player to unrank player) = 12 - 2 = 10\n",
    "* Spread(from unrank player to 3rd rank player) = 3 - 12 = -9\n",
    "\n",
    "The discrete space for ranking spread is then **[-11,-10,-9,...,9,10,11]**\n",
    "\n",
    "A match (which has at most 3 games) has 6 possible outcomes:\n",
    "1. Lose Lose     -> Lose\n",
    "2. Lose Win Lose -> Lose\n",
    "3. Win Lose Lose -> Lose\n",
    "4. Lose Win Win  -> Win\n",
    "5. Win Lose Win  -> Win\n",
    "6. Win Win       -> Win \n",
    "\n",
    "To discretize this parameter, we map the outcome of a match to **[1,2,3,4,5,6]** in terms of win degree (i.e. win degree 1 is the worst, and win degree 6 is the best)\n",
    "\n",
    "\n",
    "For the match below, ranking spread is 11, win degree is 4\n",
    "\n",
    "<img src=\"match_beautified.png\" alt=\"match\" style=\"width: 600px; margin-left: 0\"/>\n",
    "\n",
    "### 2.2 Modeling the problem\n",
    "\n",
    "Unless stated otherwise, all information in the data are from 1st player perspective\n",
    "An observation of a match includes 2 pieces of information:\n",
    "* ranking spread\n",
    "* win degree\n",
    "\n",
    "To formulate the observations as a one dimensional space collection, we need to reduce the matrix 23x6 (23 different spreads, 6 different win degrees) of all possible raw observations. The intuition of the reduction is as follows:\n",
    "* With the same ranking spread, higher win degree correlates to higher value (see arrow A in image below)\n",
    "* With the same win degree, lower ranking spread correlates to higher value (see arrow B in image below)\n",
    "* Step between value is 1\n",
    "\n",
    "With the given constraint, we will have **46 possible values of observation from [1,46]**. The mapping is as follow (columns are win degrees and rows are ranking spreads)\n",
    "\n",
    "<img src=\"mapping3.jpg\" alt=\"mapping\" style=\"width: 400px; margin-left: 0\"/>\n",
    "\n",
    "### 2.3 Analysing the problem\n",
    "\n",
    "The analysis problem is the distribution of observations and how they differ between different tournaments. Furthermore, we will also try and the analyze the affect of different models and priors choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dataset and data model\n",
    "\n",
    "The dataset is collected from Badminton World Federation (BWF) tournament database using Scrapy crawler. After collecting, the data is preprocessed as stated in the previous section. In the end, the format of data is similar to the factory data assignments. A peek of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament 1</th>\n",
       "      <th>Tournament 2</th>\n",
       "      <th>Tournament 3</th>\n",
       "      <th>Tournament 4</th>\n",
       "      <th>Tournament 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Match 1</th>\n",
       "      <td>24.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 3</th>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Match 5</th>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tournament 1  Tournament 2  Tournament 3  Tournament 4  Tournament 5\n",
       "Match 1          24.0          35.0          34.0          24.0          19.0\n",
       "Match 2          33.0          33.0          35.0          31.0          35.0\n",
       "Match 3          34.0          25.0          22.0          35.0          23.0\n",
       "Match 4          33.0          31.0          35.0          35.0          31.0\n",
       "Match 5          37.0          38.0          38.0          33.0          35.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_first_rows_of_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament 1</th>\n",
       "      <th>Tournament 2</th>\n",
       "      <th>Tournament 3</th>\n",
       "      <th>Tournament 4</th>\n",
       "      <th>Tournament 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.776119</td>\n",
       "      <td>32.388060</td>\n",
       "      <td>31.746269</td>\n",
       "      <td>32.880597</td>\n",
       "      <td>32.164179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.746727</td>\n",
       "      <td>5.635266</td>\n",
       "      <td>5.329572</td>\n",
       "      <td>5.878885</td>\n",
       "      <td>5.703792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tournament 1  Tournament 2  Tournament 3  Tournament 4  Tournament 5\n",
       "count     67.000000     67.000000     67.000000     67.000000     67.000000\n",
       "mean      31.776119     32.388060     31.746269     32.880597     32.164179\n",
       "std        5.746727      5.635266      5.329572      5.878885      5.703792\n",
       "min       22.000000     21.000000     22.000000     21.000000     19.000000\n",
       "25%       27.000000     28.000000     26.000000     28.000000     28.000000\n",
       "50%       33.000000     33.000000     33.000000     34.000000     34.000000\n",
       "75%       35.000000     36.000000     35.000000     35.000000     35.000000\n",
       "max       43.000000     45.000000     44.000000     45.000000     45.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_summary_of_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prior choices\n",
    "\n",
    "We decided to use two different priors: \n",
    "* **Inverse gamma** is chosen on variance because it is the conjugate prior to normal likelihood and it has a closed form solution for the outcome of the posterior\n",
    "* **Uniform** is chosen as weak prior to observe how sensitive is outcome in regards the prior and the data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Model\n",
    "\n",
    "In normal distribution where $\\mu$ is known and $\\sigma^2$ is unknown, the marginal posterior distribution $p(\\sigma^2|y)$ can be computed as described below. The posterior distribution is computed using two different priors, whereas the first is an uniformative (uniform) and the second an informative (inverse gamma) prior.\n",
    "\n",
    "__Priors:__\n",
    "\n",
    "Uniform prior\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\sigma^2) \\propto Uniform(0, \\infty)\n",
    "\\end{equation*}\n",
    "\n",
    "Inverse gamma prior\n",
    "\n",
    "\\begin{equation*}  \n",
    "p(\\sigma^2) \\propto (\\sigma^2)^{-(a+1)}e^{-\\beta/\\sigma^2} \\propto Inv-Gamma(\\alpha, \\beta) \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are the shape and scale parameters. __The values of the parameters ??__\n",
    "\n",
    "__Likelihood:__\n",
    "\\begin{equation*}\n",
    "p(y|\\mu,\\sigma^2) \\propto \\prod_{i=1}^{N} p(y_i | \\mu, \\sigma^2) \\propto N(y | \\mu, \\sigma^2) \n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mu$ is known.\n",
    "\n",
    "__Posterior:__\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\sigma^2 | y) \\propto p(\\sigma^2)p(y|\\mu, \\sigma^2)\n",
    "\\end{equation*}\n",
    "\n",
    "Since one of the objective is to predict the distribution of a new tournament, we will use pooled and hierarchical model. The separate model is excluded because it handles the tournaments uniquely  without having any common parameters which could be used to predict the new tournament.\n",
    "\n",
    "In the pooled model the mean and the variance is computed from the combined data of all the tournaments and there is no distinction between different tournaments. This means that also the new tournament will have similar distibution as the predictive distribution of the tournaments.  \n",
    "\n",
    "In the hierarchical model each tournament is handled separately having own mean and common standard deviation. Furthermore, all the means are controlled by common hyperparameters ($\\mu_0$ and $\\sigma^2_0$) which means that the means are drawn from the common distribution described by these hyperparameters. The result of the new tournament can be predicted using the common hyperparameters: first draw the mean from the common distribution and use it to sample the predictive distribution. \n",
    "\n",
    "Then based on the prior choices, we have 4 different models:\n",
    "* pooled with uniform prior\n",
    "* pooled with inverse gamma prior for variance\n",
    "* hierarchical with uniform prior\n",
    "* hierarchical with inverse gamma prior for variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Stan analysis of the models\n",
    "\n",
    "For each model, we will show:\n",
    "* Stan model\n",
    "* Convergence diagnostics (rhat, divergences, neff)\n",
    "\n",
    "Each model's fitting uses Stan's default parameters (4 chains, 1000 warmup iterations, 1000 sampling iteration, ending up to 4000 samples and 10 as maximum tree depth). To avoid false positive conclusion about divergences, the *adapt_delta* value is set to 0.9. This means that the fitting uses larger target acceptance probability and therefore all the divergences can be seen. If the resulting value is still 0 after this, we can verify that there are no divergences. If not, the divergences could be furter analyzed by increasing *adapt_delta*.\n",
    "\n",
    "__Everything is fine based on these diagnostics and we can proceed with our analysis.__\n",
    "\n",
    "### 6.1 Pooled model with uniform prior\n",
    "\n",
    "#### Stan model\n",
    "\n",
    "The stan code of the model:\n",
    "```\n",
    "fsk\n",
    "```\n",
    "#### Convergence diagnostic\n",
    "After compiling the model and fitting the combined data of the tournaments, the diagnostic of the fit was examined:\n",
    "* The $\\hat R$ values of the parameters are close to 1 which describes that the fit was good. The low $\\hat R$ values combined with high effective sample size (n_eff) per transition informs that the Markov chains were mixed well. \n",
    "* The value of the diveregences is 0 which informs that the Markov chains were able to explore the posterior.\n",
    "* The value of tree depth is 0 which infroms that model is identified and the sampler is efficient.\n",
    "* The value of the energy Bayesian fraction of missing information (E-BFMI) is missing which means that that the exploration of the posterior was fast (the jumps between slices of the parameter space explored by the Hamiltonian trajectories were fast).\n",
    "\n",
    "All the results for pooled uniform prior model fitting are good.\n",
    "\n",
    "__CHECK FROM ASSIGNMENT__\n",
    "\n",
    "The full fit can be found in the _Attachment 1_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>32.1949</td>\n",
       "      <td>0.00560523</td>\n",
       "      <td>0.314143</td>\n",
       "      <td>31.5688</td>\n",
       "      <td>31.9856</td>\n",
       "      <td>32.1993</td>\n",
       "      <td>32.4013</td>\n",
       "      <td>32.8212</td>\n",
       "      <td>3141</td>\n",
       "      <td>1.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>5.66357</td>\n",
       "      <td>0.00441617</td>\n",
       "      <td>0.226175</td>\n",
       "      <td>5.25044</td>\n",
       "      <td>5.50777</td>\n",
       "      <td>5.65362</td>\n",
       "      <td>5.81066</td>\n",
       "      <td>6.12685</td>\n",
       "      <td>2623</td>\n",
       "      <td>1.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[0]</th>\n",
       "      <td>-3.70566</td>\n",
       "      <td>0.00165465</td>\n",
       "      <td>0.0923644</td>\n",
       "      <td>-3.89379</td>\n",
       "      <td>-3.76673</td>\n",
       "      <td>-3.70203</td>\n",
       "      <td>-3.64212</td>\n",
       "      <td>-3.53527</td>\n",
       "      <td>3116</td>\n",
       "      <td>0.999738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[1]</th>\n",
       "      <td>-2.66387</td>\n",
       "      <td>0.000782296</td>\n",
       "      <td>0.0400502</td>\n",
       "      <td>-2.74168</td>\n",
       "      <td>-2.69029</td>\n",
       "      <td>-2.66291</td>\n",
       "      <td>-2.63689</td>\n",
       "      <td>-2.58669</td>\n",
       "      <td>2621</td>\n",
       "      <td>1.00059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[2]</th>\n",
       "      <td>-2.70474</td>\n",
       "      <td>0.000775921</td>\n",
       "      <td>0.0403031</td>\n",
       "      <td>-2.78298</td>\n",
       "      <td>-2.73239</td>\n",
       "      <td>-2.70432</td>\n",
       "      <td>-2.67709</td>\n",
       "      <td>-2.62805</td>\n",
       "      <td>2698</td>\n",
       "      <td>1.00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[332]</th>\n",
       "      <td>-2.70474</td>\n",
       "      <td>0.000775921</td>\n",
       "      <td>0.0403031</td>\n",
       "      <td>-2.78298</td>\n",
       "      <td>-2.73239</td>\n",
       "      <td>-2.70432</td>\n",
       "      <td>-2.67709</td>\n",
       "      <td>-2.62805</td>\n",
       "      <td>2698</td>\n",
       "      <td>1.00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[333]</th>\n",
       "      <td>-2.81365</td>\n",
       "      <td>0.000788474</td>\n",
       "      <td>0.0411293</td>\n",
       "      <td>-2.89514</td>\n",
       "      <td>-2.84058</td>\n",
       "      <td>-2.81256</td>\n",
       "      <td>-2.78528</td>\n",
       "      <td>-2.73606</td>\n",
       "      <td>2721</td>\n",
       "      <td>1.00086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[334]</th>\n",
       "      <td>-3.46461</td>\n",
       "      <td>0.0013434</td>\n",
       "      <td>0.0752786</td>\n",
       "      <td>-3.6169</td>\n",
       "      <td>-3.5144</td>\n",
       "      <td>-3.46345</td>\n",
       "      <td>-3.41195</td>\n",
       "      <td>-3.32293</td>\n",
       "      <td>3140</td>\n",
       "      <td>0.999774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred</th>\n",
       "      <td>32.2848</td>\n",
       "      <td>0.0884854</td>\n",
       "      <td>5.59631</td>\n",
       "      <td>21.3153</td>\n",
       "      <td>28.5981</td>\n",
       "      <td>32.3413</td>\n",
       "      <td>36.1368</td>\n",
       "      <td>43.2205</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-746.034</td>\n",
       "      <td>0.0262141</td>\n",
       "      <td>1.04265</td>\n",
       "      <td>-748.883</td>\n",
       "      <td>-746.415</td>\n",
       "      <td>-745.725</td>\n",
       "      <td>-745.288</td>\n",
       "      <td>-745.021</td>\n",
       "      <td>1582</td>\n",
       "      <td>1.0025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean      se_mean         sd     2.5%      25%      50%  \\\n",
       "mu            32.1949   0.00560523   0.314143  31.5688  31.9856  32.1993   \n",
       "sigma         5.66357   0.00441617   0.226175  5.25044  5.50777  5.65362   \n",
       "log_lik[0]   -3.70566   0.00165465  0.0923644 -3.89379 -3.76673 -3.70203   \n",
       "log_lik[1]   -2.66387  0.000782296  0.0400502 -2.74168 -2.69029 -2.66291   \n",
       "log_lik[2]   -2.70474  0.000775921  0.0403031 -2.78298 -2.73239 -2.70432   \n",
       "...               ...          ...        ...      ...      ...      ...   \n",
       "log_lik[332] -2.70474  0.000775921  0.0403031 -2.78298 -2.73239 -2.70432   \n",
       "log_lik[333] -2.81365  0.000788474  0.0411293 -2.89514 -2.84058 -2.81256   \n",
       "log_lik[334] -3.46461    0.0013434  0.0752786  -3.6169  -3.5144 -3.46345   \n",
       "ypred         32.2848    0.0884854    5.59631  21.3153  28.5981  32.3413   \n",
       "lp__         -746.034    0.0262141    1.04265 -748.883 -746.415 -745.725   \n",
       "\n",
       "                  75%    97.5% n_eff      Rhat  \n",
       "mu            32.4013  32.8212  3141   1.00008  \n",
       "sigma         5.81066  6.12685  2623    1.0007  \n",
       "log_lik[0]   -3.64212 -3.53527  3116  0.999738  \n",
       "log_lik[1]   -2.63689 -2.58669  2621   1.00059  \n",
       "log_lik[2]   -2.67709 -2.62805  2698   1.00029  \n",
       "...               ...      ...   ...       ...  \n",
       "log_lik[332] -2.67709 -2.62805  2698   1.00029  \n",
       "log_lik[333] -2.78528 -2.73606  2721   1.00086  \n",
       "log_lik[334] -3.41195 -3.32293  3140  0.999774  \n",
       "ypred         36.1368  43.2205  4000  0.999686  \n",
       "lp__         -745.288 -745.021  1582    1.0025  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pooled uniform model\n",
    "pool_uni_df, pool_uni_fit = compute_model(r'stan_code/pool_uniform_prior.stan', pooled_data_model)\n",
    "# Print summary of the fit\n",
    "print_compact_fit(pool_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergences:\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "\n",
      "Tree depth:\n",
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "\n",
      "E-BMFI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(pool_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 6.2 Pooled model with inverse gamma prior\n",
    "\n",
    "#### Stan model\n",
    "The stan code of the model:\n",
    "```\n",
    "fsk\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic\n",
    "The same procedure is followed here (as in the previous section) and similar results were obtained:\n",
    "* The $\\hat R$ values of the parameters are close to 1 which describes that the fit was good. The low $\\hat R$ values combined with high effective sample size (n_eff) per transition informs that the Markov chains were mixed well. \n",
    "* The value of the the divergences is 0\n",
    "* The value of the tree depth is 0 \n",
    "* The value of the E-BFMI is missing \n",
    "\n",
    "All of these verify that pooled inverse prior model fitting is successful.\n",
    "\n",
    "The full fit can be found in the _Attachment 2_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>32.1793</td>\n",
       "      <td>0.00689492</td>\n",
       "      <td>0.308581</td>\n",
       "      <td>31.5691</td>\n",
       "      <td>31.9764</td>\n",
       "      <td>32.1832</td>\n",
       "      <td>32.3895</td>\n",
       "      <td>32.7701</td>\n",
       "      <td>2003</td>\n",
       "      <td>1.00044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigmaSq</th>\n",
       "      <td>31.858</td>\n",
       "      <td>0.0473801</td>\n",
       "      <td>2.50443</td>\n",
       "      <td>27.362</td>\n",
       "      <td>30.0984</td>\n",
       "      <td>31.7234</td>\n",
       "      <td>33.4821</td>\n",
       "      <td>37.0895</td>\n",
       "      <td>2794</td>\n",
       "      <td>0.999906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma</th>\n",
       "      <td>5.63996</td>\n",
       "      <td>0.00417828</td>\n",
       "      <td>0.221133</td>\n",
       "      <td>5.23087</td>\n",
       "      <td>5.4862</td>\n",
       "      <td>5.63235</td>\n",
       "      <td>5.78637</td>\n",
       "      <td>6.09012</td>\n",
       "      <td>2801</td>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[0]</th>\n",
       "      <td>-3.7059</td>\n",
       "      <td>0.00200909</td>\n",
       "      <td>0.0900062</td>\n",
       "      <td>-3.88675</td>\n",
       "      <td>-3.76447</td>\n",
       "      <td>-3.70524</td>\n",
       "      <td>-3.64415</td>\n",
       "      <td>-3.53603</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.00084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[1]</th>\n",
       "      <td>-2.66019</td>\n",
       "      <td>0.000746751</td>\n",
       "      <td>0.0391457</td>\n",
       "      <td>-2.73674</td>\n",
       "      <td>-2.68703</td>\n",
       "      <td>-2.66003</td>\n",
       "      <td>-2.63316</td>\n",
       "      <td>-2.58449</td>\n",
       "      <td>2748</td>\n",
       "      <td>1.00024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[332]</th>\n",
       "      <td>-2.7019</td>\n",
       "      <td>0.000765716</td>\n",
       "      <td>0.0392163</td>\n",
       "      <td>-2.7779</td>\n",
       "      <td>-2.72741</td>\n",
       "      <td>-2.70094</td>\n",
       "      <td>-2.67566</td>\n",
       "      <td>-2.62586</td>\n",
       "      <td>2623</td>\n",
       "      <td>1.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[333]</th>\n",
       "      <td>-2.80913</td>\n",
       "      <td>0.000834766</td>\n",
       "      <td>0.0409886</td>\n",
       "      <td>-2.89114</td>\n",
       "      <td>-2.83612</td>\n",
       "      <td>-2.80866</td>\n",
       "      <td>-2.78006</td>\n",
       "      <td>-2.72968</td>\n",
       "      <td>2411</td>\n",
       "      <td>0.999903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_lik[334]</th>\n",
       "      <td>-3.46338</td>\n",
       "      <td>0.00166824</td>\n",
       "      <td>0.0735351</td>\n",
       "      <td>-3.61101</td>\n",
       "      <td>-3.51234</td>\n",
       "      <td>-3.46225</td>\n",
       "      <td>-3.4125</td>\n",
       "      <td>-3.32343</td>\n",
       "      <td>1943</td>\n",
       "      <td>1.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred</th>\n",
       "      <td>32.2442</td>\n",
       "      <td>0.0901159</td>\n",
       "      <td>5.69943</td>\n",
       "      <td>21.0271</td>\n",
       "      <td>28.471</td>\n",
       "      <td>32.3129</td>\n",
       "      <td>36.0515</td>\n",
       "      <td>43.5248</td>\n",
       "      <td>4000</td>\n",
       "      <td>1.00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-751.224</td>\n",
       "      <td>0.0253477</td>\n",
       "      <td>1.00819</td>\n",
       "      <td>-753.938</td>\n",
       "      <td>-751.639</td>\n",
       "      <td>-750.906</td>\n",
       "      <td>-750.502</td>\n",
       "      <td>-750.231</td>\n",
       "      <td>1582</td>\n",
       "      <td>0.999974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean      se_mean         sd     2.5%      25%      50%  \\\n",
       "mu            32.1793   0.00689492   0.308581  31.5691  31.9764  32.1832   \n",
       "sigmaSq        31.858    0.0473801    2.50443   27.362  30.0984  31.7234   \n",
       "sigma         5.63996   0.00417828   0.221133  5.23087   5.4862  5.63235   \n",
       "log_lik[0]    -3.7059   0.00200909  0.0900062 -3.88675 -3.76447 -3.70524   \n",
       "log_lik[1]   -2.66019  0.000746751  0.0391457 -2.73674 -2.68703 -2.66003   \n",
       "...               ...          ...        ...      ...      ...      ...   \n",
       "log_lik[332]  -2.7019  0.000765716  0.0392163  -2.7779 -2.72741 -2.70094   \n",
       "log_lik[333] -2.80913  0.000834766  0.0409886 -2.89114 -2.83612 -2.80866   \n",
       "log_lik[334] -3.46338   0.00166824  0.0735351 -3.61101 -3.51234 -3.46225   \n",
       "ypred         32.2442    0.0901159    5.69943  21.0271   28.471  32.3129   \n",
       "lp__         -751.224    0.0253477    1.00819 -753.938 -751.639 -750.906   \n",
       "\n",
       "                  75%    97.5% n_eff      Rhat  \n",
       "mu            32.3895  32.7701  2003   1.00044  \n",
       "sigmaSq       33.4821  37.0895  2794  0.999906  \n",
       "sigma         5.78637  6.09012  2801  0.999948  \n",
       "log_lik[0]   -3.64415 -3.53603  2007   1.00084  \n",
       "log_lik[1]   -2.63316 -2.58449  2748   1.00024  \n",
       "...               ...      ...   ...       ...  \n",
       "log_lik[332] -2.67566 -2.62586  2623    1.0005  \n",
       "log_lik[333] -2.78006 -2.72968  2411  0.999903  \n",
       "log_lik[334]  -3.4125 -3.32343  1943    1.0008  \n",
       "ypred         36.0515  43.5248  4000   1.00019  \n",
       "lp__         -750.502 -750.231  1582  0.999974  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pooled inverse gamma model\n",
    "pool_inv_df, pool_inv_fit = compute_model(r'stan_code/pool_inverse_gamma_prior.stan', pooled_data_model)\n",
    "# Print summary of the fit\n",
    "print_compact_fit(pool_inv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergences:\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "\n",
      "Tree depth:\n",
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "\n",
      "E-BMFI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(pool_inv_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Hierarchical model with uniform prior\n",
    "\n",
    "#### Stan model\n",
    "The stan code of the model:\n",
    "```\n",
    "fsk\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic attempt 1\n",
    "The same procedure is followed here (as in the previous section) with minor change. The data set used for fitting is a matrix where columns are the tournaments and rows are the matches in the tournaments. The diagnostic results are:\n",
    "* The $\\hat R$ values of the parameters are close to 1 (and below 1.1) which describes that the fit was good. As well, the low $\\hat R$ values combined with high effective sample size (n_eff) per transition informs that the Markov chains were mixed well.\n",
    "* The value of the the divergences is 75 which means that 2% of the target posterior was not explored by the Markov chains.\n",
    "* The value of the tree depth is 0 \n",
    "* The value of the E-BFMI is missing \n",
    "\n",
    "Because the divergences was too high, in the next step we will try to improve the results by reducing the accuracy of the simulations by adjusting the *adapt_delta* parameter.\n",
    "\n",
    "The results of the attempt 1 can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>32.1793</td>\n",
       "      <td>0.0123522</td>\n",
       "      <td>0.464316</td>\n",
       "      <td>31.2665</td>\n",
       "      <td>31.9203</td>\n",
       "      <td>32.1789</td>\n",
       "      <td>32.4496</td>\n",
       "      <td>33.0974</td>\n",
       "      <td>1413</td>\n",
       "      <td>1.00197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.0224654</td>\n",
       "      <td>0.606149</td>\n",
       "      <td>0.0638678</td>\n",
       "      <td>0.233114</td>\n",
       "      <td>0.423122</td>\n",
       "      <td>0.753836</td>\n",
       "      <td>2.07877</td>\n",
       "      <td>728</td>\n",
       "      <td>1.00319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[0]</th>\n",
       "      <td>32.0484</td>\n",
       "      <td>0.0122383</td>\n",
       "      <td>0.491822</td>\n",
       "      <td>30.9904</td>\n",
       "      <td>31.7589</td>\n",
       "      <td>32.0854</td>\n",
       "      <td>32.3603</td>\n",
       "      <td>32.9556</td>\n",
       "      <td>1615</td>\n",
       "      <td>1.00337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[1]</th>\n",
       "      <td>32.2613</td>\n",
       "      <td>0.0105294</td>\n",
       "      <td>0.461614</td>\n",
       "      <td>31.3516</td>\n",
       "      <td>31.9774</td>\n",
       "      <td>32.2519</td>\n",
       "      <td>32.5348</td>\n",
       "      <td>33.2434</td>\n",
       "      <td>1922</td>\n",
       "      <td>1.00052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[2]</th>\n",
       "      <td>32.0402</td>\n",
       "      <td>0.0126773</td>\n",
       "      <td>0.482736</td>\n",
       "      <td>31.0076</td>\n",
       "      <td>31.7547</td>\n",
       "      <td>32.073</td>\n",
       "      <td>32.352</td>\n",
       "      <td>32.9207</td>\n",
       "      <td>1450</td>\n",
       "      <td>1.00244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>32.3063</td>\n",
       "      <td>0.0896909</td>\n",
       "      <td>5.67255</td>\n",
       "      <td>21.0681</td>\n",
       "      <td>28.3453</td>\n",
       "      <td>32.2987</td>\n",
       "      <td>36.1136</td>\n",
       "      <td>43.6567</td>\n",
       "      <td>4000</td>\n",
       "      <td>1.00016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>32.1632</td>\n",
       "      <td>0.0916874</td>\n",
       "      <td>5.73614</td>\n",
       "      <td>20.9252</td>\n",
       "      <td>28.3002</td>\n",
       "      <td>32.2326</td>\n",
       "      <td>35.9441</td>\n",
       "      <td>43.3436</td>\n",
       "      <td>3914</td>\n",
       "      <td>1.00061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>32.1897</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.954034</td>\n",
       "      <td>30.3259</td>\n",
       "      <td>31.8032</td>\n",
       "      <td>32.1852</td>\n",
       "      <td>32.5619</td>\n",
       "      <td>34.0576</td>\n",
       "      <td>2722</td>\n",
       "      <td>0.999919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>32.2443</td>\n",
       "      <td>0.0943861</td>\n",
       "      <td>5.87928</td>\n",
       "      <td>21.1567</td>\n",
       "      <td>28.2786</td>\n",
       "      <td>32.2602</td>\n",
       "      <td>36.2773</td>\n",
       "      <td>43.6389</td>\n",
       "      <td>3880</td>\n",
       "      <td>1.00048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-744.586</td>\n",
       "      <td>0.205799</td>\n",
       "      <td>3.86113</td>\n",
       "      <td>-752.217</td>\n",
       "      <td>-747.178</td>\n",
       "      <td>-744.669</td>\n",
       "      <td>-742.122</td>\n",
       "      <td>-736.211</td>\n",
       "      <td>352</td>\n",
       "      <td>1.00681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean    se_mean        sd       2.5%       25%       50%  \\\n",
       "mu0         32.1793  0.0123522  0.464316    31.2665   31.9203   32.1789   \n",
       "sigma0     0.595743  0.0224654  0.606149  0.0638678  0.233114  0.423122   \n",
       "mu[0]       32.0484  0.0122383  0.491822    30.9904   31.7589   32.0854   \n",
       "mu[1]       32.2613  0.0105294  0.461614    31.3516   31.9774   32.2519   \n",
       "mu[2]       32.0402  0.0126773  0.482736    31.0076   31.7547    32.073   \n",
       "...             ...        ...       ...        ...       ...       ...   \n",
       "ypred[3]    32.3063  0.0896909   5.67255    21.0681   28.3453   32.2987   \n",
       "ypred[4]    32.1632  0.0916874   5.73614    20.9252   28.3002   32.2326   \n",
       "mu_new      32.1897   0.018286  0.954034    30.3259   31.8032   32.1852   \n",
       "ypred_new   32.2443  0.0943861   5.87928    21.1567   28.2786   32.2602   \n",
       "lp__       -744.586   0.205799   3.86113   -752.217  -747.178  -744.669   \n",
       "\n",
       "                75%    97.5% n_eff      Rhat  \n",
       "mu0         32.4496  33.0974  1413   1.00197  \n",
       "sigma0     0.753836  2.07877   728   1.00319  \n",
       "mu[0]       32.3603  32.9556  1615   1.00337  \n",
       "mu[1]       32.5348  33.2434  1922   1.00052  \n",
       "mu[2]        32.352  32.9207  1450   1.00244  \n",
       "...             ...      ...   ...       ...  \n",
       "ypred[3]    36.1136  43.6567  4000   1.00016  \n",
       "ypred[4]    35.9441  43.3436  3914   1.00061  \n",
       "mu_new      32.5619  34.0576  2722  0.999919  \n",
       "ypred_new   36.2773  43.6389  3880   1.00048  \n",
       "lp__       -742.122 -736.211   352   1.00681  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_uni_df, hier_uni_fit = compute_model(r'stan_code/hier_uniform_prior.stan', hierarchical_data_model)\n",
    "# Print the summary of the fit\n",
    "print_compact_fit(hier_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergences:\n",
      "75.0 of 4000 iterations ended with a divergence (1.875%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n",
      "Tree depth:\n",
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "\n",
      "E-BMFI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence diagnostic attempt 2\n",
    "\n",
    "After changing the accuracy of the simulations from 0.9 to 0.96, the data is re-fit and following results were gained:\n",
    "* The $\\hat R$ values of the parameters are close to 1 (and below 1.1) which describes that the fit was good. As well, the low $\\hat R$ values combined with high effective sample size (n_eff) per transition informs that the Markov chains were mixed well.\n",
    "* The value of the the divergences dropped to 1% which means that the Markov chains are exporing the posterior much better.\n",
    "* The value of the tree depth is 0 \n",
    "* The value of the E-BFMI is missing \n",
    "\n",
    "With these results we can verify that the hierarchical uniform prior model fitting is 99% successful. __I think we cannot use this model at all.... Should we try to modify the stan model????__\n",
    "\n",
    "The full fit can be found in the _Attachment 3_ and a shorter summary of attempt 2 is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>32.1923</td>\n",
       "      <td>0.0127379</td>\n",
       "      <td>0.483707</td>\n",
       "      <td>31.2362</td>\n",
       "      <td>31.9264</td>\n",
       "      <td>32.1934</td>\n",
       "      <td>32.4579</td>\n",
       "      <td>33.1532</td>\n",
       "      <td>1442</td>\n",
       "      <td>1.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.60145</td>\n",
       "      <td>0.0202741</td>\n",
       "      <td>0.612936</td>\n",
       "      <td>0.0703005</td>\n",
       "      <td>0.23329</td>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.758839</td>\n",
       "      <td>2.16422</td>\n",
       "      <td>914</td>\n",
       "      <td>1.00115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[0]</th>\n",
       "      <td>32.0521</td>\n",
       "      <td>0.0116293</td>\n",
       "      <td>0.488015</td>\n",
       "      <td>31.02</td>\n",
       "      <td>31.7594</td>\n",
       "      <td>32.0775</td>\n",
       "      <td>32.3688</td>\n",
       "      <td>32.9608</td>\n",
       "      <td>1761</td>\n",
       "      <td>1.00112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[1]</th>\n",
       "      <td>32.2586</td>\n",
       "      <td>0.0119162</td>\n",
       "      <td>0.479766</td>\n",
       "      <td>31.306</td>\n",
       "      <td>31.953</td>\n",
       "      <td>32.2553</td>\n",
       "      <td>32.5444</td>\n",
       "      <td>33.2942</td>\n",
       "      <td>1621</td>\n",
       "      <td>1.00058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[2]</th>\n",
       "      <td>32.0339</td>\n",
       "      <td>0.0124034</td>\n",
       "      <td>0.504134</td>\n",
       "      <td>30.9425</td>\n",
       "      <td>31.7535</td>\n",
       "      <td>32.0615</td>\n",
       "      <td>32.3694</td>\n",
       "      <td>32.9244</td>\n",
       "      <td>1652</td>\n",
       "      <td>1.00104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>32.4268</td>\n",
       "      <td>0.08923</td>\n",
       "      <td>5.6434</td>\n",
       "      <td>21.4007</td>\n",
       "      <td>28.6964</td>\n",
       "      <td>32.4846</td>\n",
       "      <td>36.136</td>\n",
       "      <td>43.5792</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>32.1169</td>\n",
       "      <td>0.0905623</td>\n",
       "      <td>5.72766</td>\n",
       "      <td>20.6597</td>\n",
       "      <td>28.3104</td>\n",
       "      <td>32.2082</td>\n",
       "      <td>35.9333</td>\n",
       "      <td>43.1799</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.999926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>32.1926</td>\n",
       "      <td>0.0176988</td>\n",
       "      <td>0.930659</td>\n",
       "      <td>30.3461</td>\n",
       "      <td>31.8076</td>\n",
       "      <td>32.1933</td>\n",
       "      <td>32.5902</td>\n",
       "      <td>33.9828</td>\n",
       "      <td>2765</td>\n",
       "      <td>1.00043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>32.241</td>\n",
       "      <td>0.0949073</td>\n",
       "      <td>5.79168</td>\n",
       "      <td>20.8495</td>\n",
       "      <td>28.4127</td>\n",
       "      <td>32.236</td>\n",
       "      <td>36.1046</td>\n",
       "      <td>43.4708</td>\n",
       "      <td>3724</td>\n",
       "      <td>0.999555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-744.682</td>\n",
       "      <td>0.191895</td>\n",
       "      <td>3.75056</td>\n",
       "      <td>-752.282</td>\n",
       "      <td>-747.066</td>\n",
       "      <td>-744.758</td>\n",
       "      <td>-742.256</td>\n",
       "      <td>-737.147</td>\n",
       "      <td>382</td>\n",
       "      <td>1.00997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean    se_mean        sd       2.5%      25%       50%  \\\n",
       "mu0        32.1923  0.0127379  0.483707    31.2362  31.9264   32.1934   \n",
       "sigma0     0.60145  0.0202741  0.612936  0.0703005  0.23329  0.435138   \n",
       "mu[0]      32.0521  0.0116293  0.488015      31.02  31.7594   32.0775   \n",
       "mu[1]      32.2586  0.0119162  0.479766     31.306   31.953   32.2553   \n",
       "mu[2]      32.0339  0.0124034  0.504134    30.9425  31.7535   32.0615   \n",
       "...            ...        ...       ...        ...      ...       ...   \n",
       "ypred[3]   32.4268    0.08923    5.6434    21.4007  28.6964   32.4846   \n",
       "ypred[4]   32.1169  0.0905623   5.72766    20.6597  28.3104   32.2082   \n",
       "mu_new     32.1926  0.0176988  0.930659    30.3461  31.8076   32.1933   \n",
       "ypred_new   32.241  0.0949073   5.79168    20.8495  28.4127    32.236   \n",
       "lp__      -744.682   0.191895   3.75056   -752.282 -747.066  -744.758   \n",
       "\n",
       "                75%    97.5% n_eff      Rhat  \n",
       "mu0         32.4579  33.1532  1442    1.0005  \n",
       "sigma0     0.758839  2.16422   914   1.00115  \n",
       "mu[0]       32.3688  32.9608  1761   1.00112  \n",
       "mu[1]       32.5444  33.2942  1621   1.00058  \n",
       "mu[2]       32.3694  32.9244  1652   1.00104  \n",
       "...             ...      ...   ...       ...  \n",
       "ypred[3]     36.136  43.5792  4000  0.999834  \n",
       "ypred[4]    35.9333  43.1799  4000  0.999926  \n",
       "mu_new      32.5902  33.9828  2765   1.00043  \n",
       "ypred_new   36.1046  43.4708  3724  0.999555  \n",
       "lp__       -742.256 -737.147   382   1.00997  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_uni_df, hier_uni_fit = compute_model(r'stan_code/hier_uniform_prior.stan', hierarchical_data_model, adapt_delta=0.96)\n",
    "print_compact_fit(hier_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergences:\n",
      "45.0 of 4000 iterations ended with a divergence (1.125%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n",
      "Tree depth:\n",
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "\n",
      "E-BMFI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Hierarchical model with inverse gamma prior\n",
    "\n",
    "#### Stan model\n",
    "\n",
    "The stan code of the model:\n",
    "```\n",
    "fsk\n",
    "```\n",
    "\n",
    "#### Convergence diagnostic \n",
    "\n",
    "The data is fitted using 0.94 value for the *adapt_delta* and the results are as follows:\n",
    "* The $\\hat R$ values of the parameters are close to 1 (and below 1.1) which describes that the fit was good. As well, the low $\\hat R$ values combined with high effective sample size (n_eff) per transition informs that the Markov chains were mixed well.\n",
    "* The value of the the divergences dropped to 1% which means that the Markov chains are exporing the posterior much better.\n",
    "* The value of the tree depth is 0 \n",
    "* The value of the E-BFMI is missing  \n",
    "\n",
    "With these results we can verify that the hierarchical uniform prior model fitting is 99% successful. __I think we cannot use this model at all.... Should we try to modify the stan model????__\n",
    "\n",
    "The full fit can be found in the _Attachment 4_ and a shorter summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu0</th>\n",
       "      <td>32.1827</td>\n",
       "      <td>0.0130185</td>\n",
       "      <td>0.462295</td>\n",
       "      <td>31.2777</td>\n",
       "      <td>31.9272</td>\n",
       "      <td>32.1865</td>\n",
       "      <td>32.4385</td>\n",
       "      <td>33.0798</td>\n",
       "      <td>1261</td>\n",
       "      <td>1.00136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma0</th>\n",
       "      <td>0.577608</td>\n",
       "      <td>0.0223943</td>\n",
       "      <td>0.633011</td>\n",
       "      <td>0.0692888</td>\n",
       "      <td>0.218582</td>\n",
       "      <td>0.406665</td>\n",
       "      <td>0.72989</td>\n",
       "      <td>2.04751</td>\n",
       "      <td>799</td>\n",
       "      <td>1.00004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[0]</th>\n",
       "      <td>32.0703</td>\n",
       "      <td>0.0133654</td>\n",
       "      <td>0.490165</td>\n",
       "      <td>31.0218</td>\n",
       "      <td>31.7838</td>\n",
       "      <td>32.086</td>\n",
       "      <td>32.3829</td>\n",
       "      <td>32.9904</td>\n",
       "      <td>1345</td>\n",
       "      <td>1.00097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[1]</th>\n",
       "      <td>32.261</td>\n",
       "      <td>0.0108749</td>\n",
       "      <td>0.458038</td>\n",
       "      <td>31.3905</td>\n",
       "      <td>31.9531</td>\n",
       "      <td>32.251</td>\n",
       "      <td>32.5581</td>\n",
       "      <td>33.2047</td>\n",
       "      <td>1774</td>\n",
       "      <td>1.00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu[2]</th>\n",
       "      <td>32.0424</td>\n",
       "      <td>0.0118568</td>\n",
       "      <td>0.476931</td>\n",
       "      <td>30.9972</td>\n",
       "      <td>31.762</td>\n",
       "      <td>32.0664</td>\n",
       "      <td>32.3523</td>\n",
       "      <td>32.9155</td>\n",
       "      <td>1618</td>\n",
       "      <td>1.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[3]</th>\n",
       "      <td>32.4341</td>\n",
       "      <td>0.0898893</td>\n",
       "      <td>5.6851</td>\n",
       "      <td>21.2877</td>\n",
       "      <td>28.581</td>\n",
       "      <td>32.4428</td>\n",
       "      <td>36.3299</td>\n",
       "      <td>43.535</td>\n",
       "      <td>4000</td>\n",
       "      <td>1.00084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred[4]</th>\n",
       "      <td>32.1469</td>\n",
       "      <td>0.0904786</td>\n",
       "      <td>5.69512</td>\n",
       "      <td>20.8676</td>\n",
       "      <td>28.4197</td>\n",
       "      <td>32.1721</td>\n",
       "      <td>36.0097</td>\n",
       "      <td>43.3458</td>\n",
       "      <td>3962</td>\n",
       "      <td>1.00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_new</th>\n",
       "      <td>32.1816</td>\n",
       "      <td>0.0208125</td>\n",
       "      <td>1.05015</td>\n",
       "      <td>30.4033</td>\n",
       "      <td>31.7809</td>\n",
       "      <td>32.1827</td>\n",
       "      <td>32.5685</td>\n",
       "      <td>34.0621</td>\n",
       "      <td>2546</td>\n",
       "      <td>1.00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ypred_new</th>\n",
       "      <td>31.9774</td>\n",
       "      <td>0.0910429</td>\n",
       "      <td>5.75806</td>\n",
       "      <td>20.8128</td>\n",
       "      <td>28.027</td>\n",
       "      <td>31.9613</td>\n",
       "      <td>35.8319</td>\n",
       "      <td>43.1925</td>\n",
       "      <td>4000</td>\n",
       "      <td>1.00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-749.705</td>\n",
       "      <td>0.171727</td>\n",
       "      <td>3.7427</td>\n",
       "      <td>-757.147</td>\n",
       "      <td>-752.218</td>\n",
       "      <td>-749.669</td>\n",
       "      <td>-747.185</td>\n",
       "      <td>-742.548</td>\n",
       "      <td>475</td>\n",
       "      <td>1.00119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean    se_mean        sd       2.5%       25%       50%  \\\n",
       "mu0         32.1827  0.0130185  0.462295    31.2777   31.9272   32.1865   \n",
       "sigma0     0.577608  0.0223943  0.633011  0.0692888  0.218582  0.406665   \n",
       "mu[0]       32.0703  0.0133654  0.490165    31.0218   31.7838    32.086   \n",
       "mu[1]        32.261  0.0108749  0.458038    31.3905   31.9531    32.251   \n",
       "mu[2]       32.0424  0.0118568  0.476931    30.9972    31.762   32.0664   \n",
       "...             ...        ...       ...        ...       ...       ...   \n",
       "ypred[3]    32.4341  0.0898893    5.6851    21.2877    28.581   32.4428   \n",
       "ypred[4]    32.1469  0.0904786   5.69512    20.8676   28.4197   32.1721   \n",
       "mu_new      32.1816  0.0208125   1.05015    30.4033   31.7809   32.1827   \n",
       "ypred_new   31.9774  0.0910429   5.75806    20.8128    28.027   31.9613   \n",
       "lp__       -749.705   0.171727    3.7427   -757.147  -752.218  -749.669   \n",
       "\n",
       "               75%    97.5% n_eff     Rhat  \n",
       "mu0        32.4385  33.0798  1261  1.00136  \n",
       "sigma0     0.72989  2.04751   799  1.00004  \n",
       "mu[0]      32.3829  32.9904  1345  1.00097  \n",
       "mu[1]      32.5581  33.2047  1774  1.00032  \n",
       "mu[2]      32.3523  32.9155  1618  1.00002  \n",
       "...            ...      ...   ...      ...  \n",
       "ypred[3]   36.3299   43.535  4000  1.00084  \n",
       "ypred[4]   36.0097  43.3458  3962  1.00049  \n",
       "mu_new     32.5685  34.0621  2546  1.00049  \n",
       "ypred_new  35.8319  43.1925  4000  1.00032  \n",
       "lp__      -747.185 -742.548   475  1.00119  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit hierarchical uniform model\n",
    "hier_inv_df, hier_inv_fit = compute_model(r'stan_code/hier_inverse_gamma_prior.stan', \n",
    "                                          hierarchical_data_model, adapt_delta=0.94)\n",
    "print_compact_fit(hier_inv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergences:\n",
      "50.0 of 4000 iterations ended with a divergence (1.25%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "\n",
      "Tree depth:\n",
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "\n",
      "E-BMFI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_compact_fit_checking(hier_inv_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Model comparision with PSIS-LOO and $P_{LOO-CV}$ \n",
    "\n",
    "\n",
    "* Model selection according to the hightest LOO-CV sum\n",
    "* Reliability based on the _k_values: <0.7 ok, <0.5 good\n",
    "\n",
    "\n",
    "\n",
    "__TO DO!__ The PSIS-LOO values of the models can be computed using provided _psisloo_ function. The function returns observation specific _k_-values and PSIS-LOO-CV values. In addition it return the sum of the PSIS-LOO-CV values, hence the sum of the LOO log desnities:\n",
    "\n",
    "\\begin{equation*}\n",
    "lppd_{loo-cv} = \\sum_{i=1}^{N} log \\left( \\frac{1}{S} \\sum_{s=1}^{S} p(y_i|\\theta^{is}) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "The estimated effective number of parameters ($P_{LOO-CV}$) in the model is computed as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "p_{loo-cv} = lppd-lppd_{loo-cv} \n",
    "\\end{equation*}\n",
    "\n",
    "where $lppd$ is the sum of the log densities of the posterior draws:\n",
    "\\begin{equation*}\n",
    "lppd = \\sum_{i=1}^{N} log \\left( \\frac{1}{S} \\sum_{s=1}^{S} p(y_i|\\theta^{s}) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "All the PSIS-LOO values, estimated effective number of parameters and plotted _k_-values are shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Posterior predictive checking \n",
    "\n",
    "* show predictive distributions\n",
    "* show the observations of tournaments and their predictions\n",
    "* show the obesrvations of new tournment and their predictions\n",
    "\n",
    "### 8.1 Pooled uniform\n",
    "\n",
    "### 8.2 Pooled inverse gamma\n",
    "\n",
    "### 8.3 Hierarchical uniform\n",
    "\n",
    "### 8.4 Hierarchical inverse gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Note that for hierarchical models, the new predictive tournament is computed from common hyperparatemers\n",
    "# ypreds = [\n",
    "#     pool_uni_fit.extract(permuted=True)['ypred'],     #Pooled unifrom new predictive tournament\n",
    "#     pool_inv_fit.extract(permuted=True)['ypred'],     #Pooled inverse gamma new predictive tournament\n",
    "#     hier_uni_fit.extract(permuted=True)['ypred_new'], #Hierarchical unifrom new predictive tournament \n",
    "#     hier_inv_fit.extract(permuted=True)['ypred_new']  #Hierarchical inverse gamma new predictive tournament\n",
    "# ]\n",
    "# ypreds_labels = ['Pooled uniform', 'Pooled inverse gamma', \"Hierarchical uniform\", \"Hierarchical inverse gamma\"]\n",
    "\n",
    "#print(\"The predictive distributions of the models and the actual distribution\")\n",
    "#compare_new_predictive_against_actual_distribution(ypreds,pooled_data,ypreds_labels,'Original data set')\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "ypred = hier_inv_fit.extract(permuted=True)['ypred_new']\n",
    "test_data = last\n",
    "x = np.linspace(0,50, 50)\n",
    "y = stats.norm(np.mean(test_data), np.std(test_data)).pdf(x)\n",
    "plt.hist(test_data, bins=20, normed=True, alpha=0.3, label=\"6th tournament\", color=\"C0\")\n",
    "plt.plot(x,y, label=\"6th tournament normal pdf\", color=\"C0\")\n",
    "\n",
    "y2 = stats.norm(np.mean(ypred), np.std(ypred)).pdf(x)\n",
    "plt.hist(ypred, bins=20, normed=True, alpha=0.3, label=\"ypred new samples\", color=\"C1\")\n",
    "plt.plot(x,y2,label=\"ypred new normal pdf\", color=\"C1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(1, 1, sharey=True,figsize=(16,4), subplot_kw=dict(aspect='auto'))\n",
    "# fig.suptitle('Predictive distributions of the blablabla')\n",
    "\n",
    "# axes.scatter([i for i in range(len(pooled_data))],pooled_data,marker=\"o\", color=\"C1\", alpha=0.5)\n",
    "# axes.scatter([i for i in range(len(pooled_data))],pooled_data,marker=\"x\", color=\"C0\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# the predictive distribution of all the models look similar, \n",
    "# therefore the prediction of the new tournament looks similar as the one which was left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CODE EXAMPLE FOR PLOTTING SEVERAL AXES, REMOVE IT IF NOT NEEDED!!!\"\"\"\n",
    "\n",
    "#Show predictive distributions for all the machines and for new machine\n",
    "def show_predictive_distributions(samples):\n",
    "    m = 0;\n",
    "   \n",
    "    #First row of plots: machines from 1 to 4\n",
    "    fig, axes = plt.subplots(1, 4, sharey=True,figsize=(16,4), subplot_kw=dict(aspect='auto'))\n",
    "    fig.suptitle('Predictive distributions of the machines')\n",
    "    for i in range(0,4):\n",
    "        axes[i].set_title('Machine '+str(m+1))\n",
    "        mu = np.mean(samples[\"ypred\"][:,m])\n",
    "        if mu<85:\n",
    "            axes[i].hist(samples[\"ypred\"][:,m],50, density=True, alpha=0.3, color='R')\n",
    "        else:\n",
    "            axes[i].hist(samples[\"ypred\"][:,m],50, density=True, alpha=0.7)\n",
    "        axes[i].plot([85,85],[0.027, 0],'-',color='K',linewidth=1.5, label=r'$\\theta=85$')\n",
    "        axes[i].plot([mu,mu],[0.027, 0],'--',color='K', linewidth=1.5, label=r'$\\theta$ mean')\n",
    "        m+=1\n",
    "        axes[i].legend()\n",
    "    plt.show()   \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9 Sensitivity analysis (?)\n",
    "* on the priors \n",
    "* and the model (ask ta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Conclusion\n",
    "* problems: \n",
    " - data model is not 100& justifiable \n",
    "* potential improvements\n",
    " - data model can be modified so that multinomial can be used for likelihood\n",
    " - extensive experiment with binomial model\n",
    "* discussion\n",
    " - it's accurate\n",
    "* conclusion of the data analysis\n",
    "* Is there a discussion of problems and potential improvements ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pystan\n",
    "import stan_utility\n",
    "import psis\n",
    "import plot_tools\n",
    "\n",
    "# For hiding warnings that do not effect the functionality of the code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is provided as files, 1 file contains all matches result of a tournament\n",
    "# We will use data from all tournaments to fit model\n",
    "# Except for the last one which will be used to evaluate prediction accuracy\n",
    "data = []\n",
    "filenames = os.listdir(r'./data/')\n",
    "for idx, filename in enumerate(filenames):\n",
    "    col = np.loadtxt(f'data//{filename}').tolist()[0:67]\n",
    "    if (idx == (len(filenames) - 1)):\n",
    "        last = col[0:67]\n",
    "    else:\n",
    "        data.append(col)\n",
    "\n",
    "np_data = np.array(data)\n",
    "\n",
    "def show_first_rows_of_data():\n",
    "    df = pd.DataFrame(np_data.T)\n",
    "    df.columns=['Tournament '+str(i+1) for i in range(np_data.shape[0])]\n",
    "    df = df.rename({i: 'Match '+str(i+1) for i in range(np_data.shape[1])}, axis='index')\n",
    "    return df.head()\n",
    "\n",
    "def show_summary_of_data():\n",
    "    df = pd.DataFrame(np_data.T)\n",
    "    df.columns=['Tournament '+str(i+1) for i in range(np_data.shape[0])]\n",
    "    return df.describe()\n",
    "\n",
    "# print number of observations (matches) and number of predictors (tournaments)\n",
    "\n",
    "def print_model(file_path):\n",
    "    with open(file_path) as file:\n",
    "        print(file.read())\n",
    "\n",
    "pooled_data = np_data.flatten()\n",
    "pooled_data_model = dict(N=len(pooled_data), y=pooled_data, alpha=1, beta=1)\n",
    "\n",
    "pooled_inv_g_data_model = dict(N=len(pooled_data), y=pooled_data)\n",
    "\n",
    "hierarchical_data_model = dict(N = np_data.shape[1], J= np_data.shape[0],y = np_data.T, alpha=1, beta=1)\n",
    "# choosing alpha and beta?\n",
    "\n",
    "'''\n",
    "adapt_delta: 0...1\n",
    "    Effects to divergences, hence to the accuracy of the posterior. \n",
    "    The smaller the value is the more strict the Stan model is in accepting sampels.\n",
    "    The bigger the value is the easier the Stan model accepts samples. \n",
    "'''\n",
    "def compute_model(file_path, data, chains=4, iter=2000, adapt_delta=0.9):\n",
    "    # Compile model for both separated and pooled\n",
    "    model = stan_utility.compile_model(file_path) \n",
    "\n",
    "    # Fit model: adapt_delta is used for divergences\n",
    "    fit = model.sampling(data=data, seed=194838,chains=chains, iter=iter, control=dict(adapt_delta=adapt_delta))\n",
    "    #stan_utility.check_treedepth(fit)\n",
    "    #stan_utility.check_energy(fit)\n",
    "    #stan_utility.check_div(fit)\n",
    "\n",
    "    # get summary of the fit, use pandas data frame for layout\n",
    "    summary = fit.summary()\n",
    "    df = pd.DataFrame(summary['summary'], index=summary['summary_rownames'], columns=summary['summary_colnames'])\n",
    "    \n",
    "    return df, fit\n",
    "\n",
    "def print_compact_fit(fit_df, number_of_rows_head=5, number_of_rows_tail=5):\n",
    "    df = fit_df.head(number_of_rows_head)\n",
    "    df = df.append([{'mean':'...','se_mean':'...','sd':'...','2.5%':'...','25%':'...',\n",
    "                   '50%':'...','75%':'...','97.5%':'...','n_eff':'...','Rhat':'...'}])\n",
    "    df = df.rename({0: '...'}, axis='index')\n",
    "    df = df.append(fit_df.tail(number_of_rows_tail))\n",
    "    return df\n",
    "\n",
    "def print_compact_fit_checking(fit):\n",
    "    # Check divergences\n",
    "    print(\"Divergences:\")\n",
    "    stan_utility.check_div(fit)\n",
    "    print(\"\")\n",
    "\n",
    "    # Check tree depth\n",
    "    print(\"Tree depth:\")\n",
    "    stan_utility.check_treedepth(fit)\n",
    "    print(\"\")\n",
    "\n",
    "    # Check E-BMFI\n",
    "    print(\"E-BMFI:\")\n",
    "    stan_utility.check_energy(fit)\n",
    "    print(\"\")\n",
    "\n",
    "def compare_new_predictive_against_actual_distribution(new_preds, actual, new_pred_labels, actual_label):\n",
    "    fig, axes = plot_tools.hist_multi_sharex(\n",
    "        [new_preds[i] for i in range(len(new_preds))]+[actual],\n",
    "        rowlabels= [ new_pred_labels[i] for i in range(len(new_pred_labels))]+[actual_label],\n",
    "        n_bins=30,x_lines=np.mean(actual),figsize=(7, 10) )\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def compare_psis_loo(log_liks):\n",
    "    ## compute psis_loo, p_eff\n",
    "    ## plot k-values\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 1: Fit of pooled model with uniform prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pool_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 2: Fit of pooled model with inverse gamma prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pool_inv_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 3: Fit of hierarchical model with uniform prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_4dbddebd54b18baabe527e1d9f8ac5b1.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "                mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "mu0            32.15    0.03   0.44  31.32   31.9  32.12  32.42  33.06    259   1.01\n",
      "sigma0          0.61    0.04   0.51   0.15   0.27   0.44   0.75   1.93    190   1.01\n",
      "mu[0]          31.99    0.02   0.48   31.0  31.71   32.0  32.28  32.96    530   1.01\n",
      "mu[1]          32.23    0.02   0.48  31.34   31.9   32.2  32.52  33.22    381   1.01\n",
      "mu[2]           32.0    0.02   0.47  31.04  31.74   32.0  32.29  32.92    866    1.0\n",
      "mu[3]          32.38    0.04   0.52  31.43  32.01  32.32  32.71  33.53    166   1.02\n",
      "mu[4]          32.14    0.02   0.49  31.23  31.81  32.16  32.43  33.16    427   1.01\n",
      "sigma           5.66    0.02   0.22   5.28   5.51   5.64   5.79   6.09    155   1.01\n",
      "log_lik[0,0]   -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[1,0]   -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[2,0]   -2.72  2.9e-3   0.05  -2.81  -2.75  -2.72  -2.68  -2.64    255    1.0\n",
      "log_lik[3,0]   -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[4,0]   -3.05  3.2e-3   0.08  -3.22  -3.09  -3.04   -3.0  -2.91    550    1.0\n",
      "log_lik[5,0]    -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[6,0]    -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[7,0]    -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[8,0]   -3.92  7.0e-3   0.15  -4.22  -4.02  -3.92  -3.83  -3.64    436    1.0\n",
      "log_lik[9,0]    -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[10,0]   -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[11,0]  -3.05  3.4e-3   0.07  -3.21  -3.09  -3.04   -3.0   -2.9    473   1.01\n",
      "log_lik[12,0]  -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[13,0]   -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[14,0]  -3.92  7.0e-3   0.15  -4.22  -4.02  -3.92  -3.83  -3.64    436    1.0\n",
      "log_lik[15,0]  -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[16,0]   -2.8  3.6e-3   0.05  -2.91  -2.83  -2.79  -2.76   -2.7    208   1.01\n",
      "log_lik[17,0]  -3.05  3.4e-3   0.07  -3.21  -3.09  -3.04   -3.0   -2.9    473   1.01\n",
      "log_lik[18,0]  -2.72  2.9e-3   0.05  -2.81  -2.75  -2.72  -2.68  -2.64    255    1.0\n",
      "log_lik[19,0]  -3.43  4.9e-3   0.11  -3.66  -3.48  -3.42  -3.36  -3.22    473   1.01\n",
      "log_lik[20,0]   -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[21,0]  -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[22,0]  -4.23    0.01   0.17  -4.58  -4.34  -4.22  -4.11  -3.91    237   1.01\n",
      "log_lik[23,0]   -2.9  3.2e-3   0.06  -3.04  -2.94   -2.9  -2.86  -2.79    373   1.01\n",
      "log_lik[24,0]  -3.05  3.4e-3   0.07  -3.21  -3.09  -3.04   -3.0   -2.9    473   1.01\n",
      "log_lik[25,0]  -3.22  4.0e-3   0.09  -3.41  -3.27  -3.22  -3.16  -3.04    507   1.01\n",
      "log_lik[26,0]  -3.66  6.1e-3   0.13  -3.92  -3.74  -3.66  -3.58  -3.43    420   1.01\n",
      "log_lik[27,0]  -2.72  3.6e-3   0.05  -2.81  -2.75  -2.71  -2.68  -2.64    161   1.02\n",
      "log_lik[28,0]  -3.05  3.2e-3   0.08  -3.22  -3.09  -3.04   -3.0  -2.91    550    1.0\n",
      "log_lik[29,0]  -3.05  3.4e-3   0.07  -3.21  -3.09  -3.04   -3.0   -2.9    473   1.01\n",
      "log_lik[30,0]  -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[31,0]  -2.72  3.6e-3   0.05  -2.81  -2.75  -2.71  -2.68  -2.64    161   1.02\n",
      "log_lik[32,0]  -3.22  4.0e-3   0.09  -3.41  -3.27  -3.22  -3.16  -3.04    507   1.01\n",
      "log_lik[33,0]   -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[34,0]  -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[35,0]  -4.56    0.01    0.2  -4.96  -4.68  -4.56  -4.42  -4.18    216   1.01\n",
      "log_lik[36,0]  -3.05  3.2e-3   0.08  -3.22  -3.09  -3.04   -3.0  -2.91    550    1.0\n",
      "log_lik[37,0]  -4.23    0.01   0.17  -4.58  -4.34  -4.22  -4.11  -3.91    237   1.01\n",
      "log_lik[38,0]  -2.72  2.9e-3   0.05  -2.81  -2.75  -2.72  -2.68  -2.64    255    1.0\n",
      "log_lik[39,0]  -3.92  7.0e-3   0.15  -4.22  -4.02  -3.92  -3.83  -3.64    436    1.0\n",
      "log_lik[40,0]   -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[41,0]  -3.22  3.9e-3   0.09  -3.42  -3.27  -3.21  -3.16  -3.05    528   1.01\n",
      "log_lik[42,0]  -3.22  4.0e-3   0.09  -3.41  -3.27  -3.22  -3.16  -3.04    507   1.01\n",
      "log_lik[43,0]  -3.66  6.1e-3   0.13  -3.92  -3.74  -3.66  -3.58  -3.43    420   1.01\n",
      "log_lik[44,0]  -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[45,0]  -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[46,0]  -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[47,0]  -3.05  3.4e-3   0.07  -3.21  -3.09  -3.04   -3.0   -2.9    473   1.01\n",
      "log_lik[48,0]  -3.22  4.0e-3   0.09  -3.41  -3.27  -3.22  -3.16  -3.04    507   1.01\n",
      "log_lik[49,0]   -2.8  2.7e-3   0.05  -2.91  -2.83  -2.79  -2.75  -2.71    401    1.0\n",
      "log_lik[50,0]  -3.43  4.9e-3   0.11  -3.66  -3.48  -3.42  -3.36  -3.22    473   1.01\n",
      "log_lik[51,0]  -3.05  3.2e-3   0.08  -3.22  -3.09  -3.04   -3.0  -2.91    550    1.0\n",
      "log_lik[52,0]  -3.66  6.1e-3   0.13  -3.92  -3.74  -3.66  -3.58  -3.43    420   1.01\n",
      "log_lik[53,0]  -4.22  8.5e-3   0.17  -4.55  -4.35  -4.22  -4.11  -3.89    400    1.0\n",
      "log_lik[54,0]  -3.05  3.4e-3   0.07  -3.21  -3.09  -3.04   -3.0   -2.9    473   1.01\n",
      "log_lik[55,0]  -2.72  2.9e-3   0.05  -2.81  -2.75  -2.72  -2.68  -2.64    255    1.0\n",
      "log_lik[56,0]  -2.91  2.8e-3   0.06  -3.04  -2.94   -2.9  -2.86  -2.79    504    1.0\n",
      "log_lik[57,0]  -2.72  3.6e-3   0.05  -2.81  -2.75  -2.71  -2.68  -2.64    161   1.02\n",
      "log_lik[58,0]  -2.67  3.0e-3   0.04  -2.76   -2.7  -2.67  -2.64   -2.6    183   1.01\n",
      "log_lik[59,0]   -2.9  3.2e-3   0.06  -3.04  -2.94   -2.9  -2.86  -2.79    373   1.01\n",
      "log_lik[60,0]  -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[61,0]   -2.9  3.2e-3   0.06  -3.04  -2.94   -2.9  -2.86  -2.79    373   1.01\n",
      "log_lik[62,0]  -3.66  5.8e-3   0.13  -3.92  -3.74  -3.65  -3.58  -3.42    472    1.0\n",
      "log_lik[63,0]  -3.42  4.8e-3   0.11  -3.65  -3.49  -3.42  -3.35  -3.22    501    1.0\n",
      "log_lik[64,0]  -2.72  2.9e-3   0.05  -2.81  -2.75  -2.72  -2.68  -2.64    255    1.0\n",
      "log_lik[65,0]  -3.22  3.9e-3   0.09  -3.42  -3.27  -3.21  -3.16  -3.05    528   1.01\n",
      "log_lik[66,0]  -3.66  6.1e-3   0.13  -3.92  -3.74  -3.66  -3.58  -3.43    420   1.01\n",
      "log_lik[0,1]   -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[1,1]   -2.66  3.0e-3   0.04  -2.75  -2.69  -2.66  -2.64   -2.6    176   1.01\n",
      "log_lik[2,1]   -3.47  5.3e-3   0.11  -3.71  -3.54  -3.46   -3.4  -3.28    440   1.01\n",
      "log_lik[3,1]   -2.68  3.8e-3   0.04  -2.76  -2.71  -2.68  -2.65   -2.6    124   1.02\n",
      "log_lik[4,1]   -3.18  4.4e-3   0.09  -3.34  -3.24  -3.18  -3.12  -3.01    370   1.01\n",
      "log_lik[5,1]   -3.18  4.4e-3   0.09  -3.34  -3.24  -3.18  -3.12  -3.01    370   1.01\n",
      "log_lik[6,1]    -2.7  2.7e-3   0.04   -2.8  -2.73   -2.7  -2.67  -2.63    270   1.01\n",
      "log_lik[7,1]   -5.21    0.02   0.25  -5.66  -5.38  -5.21  -5.05  -4.73    133   1.01\n",
      "log_lik[8,1]   -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[9,1]   -2.73  4.2e-3   0.05  -2.83  -2.76  -2.73   -2.7  -2.65    132   1.01\n",
      "log_lik[10,1]  -2.66  3.4e-3   0.04  -2.73  -2.68  -2.65  -2.63  -2.58    134   1.01\n",
      "log_lik[11,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[12,1]  -3.37  7.2e-3    0.1  -3.56  -3.45  -3.38  -3.31  -3.18    197   1.01\n",
      "log_lik[13,1]  -3.47  5.3e-3   0.11  -3.71  -3.54  -3.46   -3.4  -3.28    440   1.01\n",
      "log_lik[14,1]  -2.82  4.5e-3   0.06  -2.94  -2.85  -2.81  -2.78  -2.73    152   1.01\n",
      "log_lik[15,1]  -3.72  6.2e-3   0.13  -3.99  -3.79  -3.71  -3.63  -3.47    446   1.01\n",
      "log_lik[16,1]  -2.68  3.8e-3   0.04  -2.76  -2.71  -2.68  -2.65   -2.6    124   1.02\n",
      "log_lik[17,1]  -3.26  4.8e-3   0.09  -3.47  -3.32  -3.25   -3.2   -3.1    394   1.01\n",
      "log_lik[18,1]  -2.73  4.2e-3   0.05  -2.83  -2.76  -2.73   -2.7  -2.65    132   1.01\n",
      "log_lik[19,1]  -3.01  3.4e-3   0.07  -3.15  -3.06  -3.01  -2.97  -2.88    429   1.01\n",
      "log_lik[20,1]  -3.18  4.4e-3   0.09  -3.34  -3.24  -3.18  -3.12  -3.01    370   1.01\n",
      "log_lik[21,1]  -2.88  2.8e-3   0.06   -3.0  -2.92  -2.87  -2.84  -2.76    447    1.0\n",
      "log_lik[22,1]  -4.15    0.01   0.16  -4.44  -4.28  -4.15  -4.04  -3.83    152   1.01\n",
      "log_lik[23,1]  -3.08  5.5e-3   0.08  -3.26  -3.13  -3.08  -3.03  -2.95    207   1.01\n",
      "log_lik[24,1]  -2.68  3.8e-3   0.04  -2.76  -2.71  -2.68  -2.65   -2.6    124   1.02\n",
      "log_lik[25,1]  -2.88  2.8e-3   0.06   -3.0  -2.92  -2.87  -2.84  -2.76    447    1.0\n",
      "log_lik[26,1]  -3.37  7.2e-3    0.1  -3.56  -3.45  -3.38  -3.31  -3.18    197   1.01\n",
      "log_lik[27,1]  -2.66  3.0e-3   0.04  -2.75  -2.69  -2.66  -2.64   -2.6    176   1.01\n",
      "log_lik[28,1]  -2.66  3.4e-3   0.04  -2.73  -2.68  -2.65  -2.63  -2.58    134   1.01\n",
      "log_lik[29,1]  -3.08  5.5e-3   0.08  -3.26  -3.13  -3.08  -3.03  -2.95    207   1.01\n",
      "log_lik[30,1]  -3.72  6.2e-3   0.13  -3.99  -3.79  -3.71  -3.63  -3.47    446   1.01\n",
      "log_lik[31,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[32,1]  -4.63    0.01    0.2  -5.05  -4.76  -4.63   -4.5  -4.24    388   1.01\n",
      "log_lik[33,1]  -2.66  3.0e-3   0.04  -2.75  -2.69  -2.66  -2.64   -2.6    176   1.01\n",
      "log_lik[34,1]   -2.7  2.7e-3   0.04   -2.8  -2.73   -2.7  -2.67  -2.63    270   1.01\n",
      "log_lik[35,1]  -3.18  4.4e-3   0.09  -3.34  -3.24  -3.18  -3.12  -3.01    370   1.01\n",
      "log_lik[36,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[37,1]  -4.48    0.02   0.19  -4.81  -4.62  -4.48  -4.35   -4.1    144   1.01\n",
      "log_lik[38,1]  -3.72  6.2e-3   0.13  -3.99  -3.79  -3.71  -3.63  -3.47    446   1.01\n",
      "log_lik[39,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[40,1]   -2.7  2.7e-3   0.04   -2.8  -2.73   -2.7  -2.67  -2.63    270   1.01\n",
      "log_lik[41,1]  -3.47  5.3e-3   0.11  -3.71  -3.54  -3.46   -3.4  -3.28    440   1.01\n",
      "log_lik[42,1]  -3.72  6.2e-3   0.13  -3.99  -3.79  -3.71  -3.63  -3.47    446   1.01\n",
      "log_lik[43,1]  -3.08  5.5e-3   0.08  -3.26  -3.13  -3.08  -3.03  -2.95    207   1.01\n",
      "log_lik[44,1]  -2.66  3.4e-3   0.04  -2.73  -2.68  -2.65  -2.63  -2.58    134   1.01\n",
      "log_lik[45,1]  -3.72  6.2e-3   0.13  -3.99  -3.79  -3.71  -3.63  -3.47    446   1.01\n",
      "log_lik[46,1]  -2.68  3.8e-3   0.04  -2.76  -2.71  -2.68  -2.65   -2.6    124   1.02\n",
      "log_lik[47,1]  -3.26  4.8e-3   0.09  -3.47  -3.32  -3.25   -3.2   -3.1    394   1.01\n",
      "log_lik[48,1]  -2.94  5.0e-3   0.07  -3.08  -2.97  -2.93  -2.88  -2.83    177   1.01\n",
      "log_lik[49,1]  -2.88  2.8e-3   0.06   -3.0  -2.92  -2.87  -2.84  -2.76    447    1.0\n",
      "log_lik[50,1]  -4.48    0.02   0.19  -4.81  -4.62  -4.48  -4.35   -4.1    144   1.01\n",
      "log_lik[51,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[52,1]   -3.6  9.0e-3   0.12  -3.82   -3.7   -3.6  -3.52  -3.36    176   1.01\n",
      "log_lik[53,1]  -2.94  5.0e-3   0.07  -3.08  -2.97  -2.93  -2.88  -2.83    177   1.01\n",
      "log_lik[54,1]  -2.66  3.0e-3   0.04  -2.75  -2.69  -2.66  -2.64   -2.6    176   1.01\n",
      "log_lik[55,1]  -3.72  6.2e-3   0.13  -3.99  -3.79  -3.71  -3.63  -3.47    446   1.01\n",
      "log_lik[56,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[57,1]  -2.82  4.5e-3   0.06  -2.94  -2.85  -2.81  -2.78  -2.73    152   1.01\n",
      "log_lik[58,1]   -3.6  9.0e-3   0.12  -3.82   -3.7   -3.6  -3.52  -3.36    176   1.01\n",
      "log_lik[59,1]  -2.88  2.8e-3   0.06   -3.0  -2.92  -2.87  -2.84  -2.76    447    1.0\n",
      "log_lik[60,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[61,1]  -2.68  3.8e-3   0.04  -2.76  -2.71  -2.68  -2.65   -2.6    124   1.02\n",
      "log_lik[62,1]   -4.3  8.8e-3   0.18  -4.67  -4.41  -4.29  -4.18  -3.96    406    1.0\n",
      "log_lik[63,1]  -2.94  5.0e-3   0.07  -3.08  -2.97  -2.93  -2.88  -2.83    177   1.01\n",
      "log_lik[64,1]  -2.88  2.8e-3   0.06   -3.0  -2.92  -2.87  -2.84  -2.76    447    1.0\n",
      "log_lik[65,1]  -3.18  4.4e-3   0.09  -3.34  -3.24  -3.18  -3.12  -3.01    370   1.01\n",
      "log_lik[66,1]  -2.78  2.5e-3   0.05  -2.88  -2.81  -2.77  -2.74  -2.68    394    1.0\n",
      "log_lik[0,2]   -2.72  3.0e-3   0.05  -2.81  -2.75  -2.72  -2.69  -2.64    234   1.01\n",
      "log_lik[1,2]    -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[2,2]   -4.22  8.1e-3   0.17  -4.55  -4.35  -4.22  -4.11  -3.91    423    1.0\n",
      "log_lik[3,2]    -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[4,2]   -3.22  3.1e-3   0.09  -3.41  -3.27  -3.22  -3.16  -3.06    849    1.0\n",
      "log_lik[5,2]    -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[6,2]   -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[7,2]    -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[8,2]    -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[9,2]    -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[10,2]  -3.92  6.5e-3   0.14   -4.2  -4.03  -3.92  -3.83  -3.65    486    1.0\n",
      "log_lik[11,2]  -2.72  3.0e-3   0.05  -2.81  -2.75  -2.72  -2.69  -2.64    234   1.01\n",
      "log_lik[12,2]  -2.72  3.0e-3   0.05  -2.81  -2.75  -2.72  -2.69  -2.64    234   1.01\n",
      "log_lik[13,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[14,2]  -2.72  3.0e-3   0.05  -2.81  -2.75  -2.72  -2.69  -2.64    234   1.01\n",
      "log_lik[15,2]  -3.66  5.0e-3   0.12   -3.9  -3.73  -3.66  -3.58  -3.42    607    1.0\n",
      "log_lik[16,2]   -2.8  2.3e-3   0.05   -2.9  -2.83  -2.79  -2.76   -2.7    523    1.0\n",
      "log_lik[17,2]  -3.05  2.6e-3   0.07   -3.2  -3.09  -3.04   -3.0  -2.91    779    1.0\n",
      "log_lik[18,2]  -2.91  2.4e-3   0.06  -3.03  -2.94   -2.9  -2.87  -2.79    696    1.0\n",
      "log_lik[19,2]  -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[20,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[21,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[22,2]  -3.66  4.6e-3   0.12  -3.91  -3.73  -3.66  -3.58  -3.43    750    1.0\n",
      "log_lik[23,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[24,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[25,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[26,2]  -3.66  4.6e-3   0.12  -3.91  -3.73  -3.66  -3.58  -3.43    750    1.0\n",
      "log_lik[27,2]  -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[28,2]  -3.93  5.6e-3   0.15  -4.22  -4.02  -3.93  -3.83  -3.66    672    1.0\n",
      "log_lik[29,2]  -3.05  2.6e-3   0.07  -3.21  -3.09  -3.04   -3.0  -2.91    805    1.0\n",
      "log_lik[30,2]  -3.66  5.0e-3   0.12   -3.9  -3.73  -3.66  -3.58  -3.42    607    1.0\n",
      "log_lik[31,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[32,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[33,2]  -3.66  5.0e-3   0.12   -3.9  -3.73  -3.66  -3.58  -3.42    607    1.0\n",
      "log_lik[34,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[35,2]  -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[36,2]  -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[37,2]  -4.92    0.01   0.22  -5.36  -5.07  -4.91  -4.76  -4.51    268   1.01\n",
      "log_lik[38,2]  -2.67  3.4e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    144   1.01\n",
      "log_lik[39,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[40,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[41,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[42,2]  -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[43,2]  -2.67  3.1e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    169   1.01\n",
      "log_lik[44,2]  -3.66  5.0e-3   0.12   -3.9  -3.73  -3.66  -3.58  -3.42    607    1.0\n",
      "log_lik[45,2]  -3.66  5.0e-3   0.12   -3.9  -3.73  -3.66  -3.58  -3.42    607    1.0\n",
      "log_lik[46,2]  -2.91  2.4e-3   0.06  -3.03  -2.94   -2.9  -2.87  -2.79    696    1.0\n",
      "log_lik[47,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[48,2]   -2.8  2.3e-3   0.05   -2.9  -2.83  -2.79  -2.76   -2.7    523    1.0\n",
      "log_lik[49,2]  -3.93  5.6e-3   0.15  -4.22  -4.02  -3.93  -3.83  -3.66    672    1.0\n",
      "log_lik[50,2]  -3.22  3.1e-3   0.09  -3.41  -3.27  -3.22  -3.16  -3.06    849    1.0\n",
      "log_lik[51,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[52,2]  -3.66  5.0e-3   0.12   -3.9  -3.73  -3.66  -3.58  -3.42    607    1.0\n",
      "log_lik[53,2]  -2.91  2.4e-3   0.06  -3.03  -2.94   -2.9  -2.87  -2.79    696    1.0\n",
      "log_lik[54,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[55,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[56,2]  -2.72  3.0e-3   0.05  -2.81  -2.75  -2.72  -2.69  -2.64    234   1.01\n",
      "log_lik[57,2]  -4.23  6.9e-3   0.17  -4.56  -4.34  -4.23  -4.11  -3.91    601    1.0\n",
      "log_lik[58,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[59,2]   -2.8  2.6e-3   0.05   -2.9  -2.83  -2.79  -2.76  -2.71    410    1.0\n",
      "log_lik[60,2]  -2.72  3.3e-3   0.05  -2.81  -2.75  -2.71  -2.69  -2.64    185   1.01\n",
      "log_lik[61,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[62,2]  -2.67  3.4e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    144   1.01\n",
      "log_lik[63,2]  -3.22  3.2e-3   0.09   -3.4  -3.27  -3.21  -3.17  -3.05    772    1.0\n",
      "log_lik[64,2]  -2.65  3.4e-3   0.04  -2.73  -2.68  -2.65  -2.63  -2.58    135   1.01\n",
      "log_lik[65,2]  -2.67  3.4e-3   0.04  -2.75   -2.7  -2.67  -2.64   -2.6    144   1.01\n",
      "log_lik[66,2]  -3.92  6.5e-3   0.14   -4.2  -4.03  -3.92  -3.83  -3.65    486    1.0\n",
      "log_lik[0,3]   -3.76  7.8e-3   0.15  -4.09  -3.83  -3.75  -3.65  -3.51    356   1.01\n",
      "log_lik[1,3]   -2.69  4.1e-3   0.04  -2.78  -2.72  -2.68  -2.66  -2.61    119   1.02\n",
      "log_lik[2,3]   -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[3,3]   -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[4,3]   -2.66  3.1e-3   0.04  -2.74  -2.69  -2.66  -2.63   -2.6    158   1.01\n",
      "log_lik[5,3]   -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[6,3]   -4.11    0.02   0.17  -4.42  -4.24   -4.1  -3.99  -3.76    126   1.02\n",
      "log_lik[7,3]   -4.77    0.02   0.23  -5.17  -4.94  -4.77  -4.62  -4.32    119   1.02\n",
      "log_lik[8,3]   -2.96  6.2e-3   0.08  -3.13   -3.0  -2.95   -2.9  -2.84    153   1.02\n",
      "log_lik[9,3]   -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[10,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[11,3]   -2.7  2.7e-3   0.04  -2.79  -2.72  -2.69  -2.67  -2.62    261   1.01\n",
      "log_lik[12,3]  -4.04  9.0e-3   0.17  -4.42  -4.13  -4.03  -3.92  -3.74    361   1.01\n",
      "log_lik[13,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[14,3]  -3.76  7.8e-3   0.15  -4.09  -3.83  -3.75  -3.65  -3.51    356   1.01\n",
      "log_lik[15,3]  -2.66  3.1e-3   0.04  -2.74  -2.69  -2.66  -2.63   -2.6    158   1.01\n",
      "log_lik[16,3]  -3.11  7.0e-3   0.09  -3.32  -3.16   -3.1  -3.04  -2.96    171   1.01\n",
      "log_lik[17,3]  -4.35    0.01    0.2  -4.78  -4.45  -4.34  -4.21   -4.0    360   1.01\n",
      "log_lik[18,3]  -2.96  6.2e-3   0.08  -3.13   -3.0  -2.95   -2.9  -2.84    153   1.02\n",
      "log_lik[19,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[20,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[21,3]  -2.66  3.1e-3   0.04  -2.74  -2.69  -2.66  -2.63   -2.6    158   1.01\n",
      "log_lik[22,3]  -3.57    0.01   0.13  -3.79  -3.67  -3.57  -3.48   -3.3    139   1.02\n",
      "log_lik[23,3]   -2.7  2.7e-3   0.04  -2.79  -2.72  -2.69  -2.67  -2.62    261   1.01\n",
      "log_lik[24,3]  -3.34  8.8e-3   0.11  -3.54  -3.42  -3.35  -3.27  -3.12    149   1.02\n",
      "log_lik[25,3]  -2.96  6.2e-3   0.08  -3.13   -3.0  -2.95   -2.9  -2.84    153   1.02\n",
      "log_lik[26,3]  -3.57    0.01   0.13  -3.79  -3.67  -3.57  -3.48   -3.3    139   1.02\n",
      "log_lik[27,3]  -2.99  4.2e-3   0.07  -3.14  -3.04  -2.99  -2.94  -2.84    314   1.01\n",
      "log_lik[28,3]  -3.34  8.8e-3   0.11  -3.54  -3.42  -3.35  -3.27  -3.12    149   1.02\n",
      "log_lik[29,3]  -2.66  3.1e-3   0.04  -2.74  -2.69  -2.66  -2.63   -2.6    158   1.01\n",
      "log_lik[30,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[31,3]   -2.7  2.7e-3   0.04  -2.79  -2.72  -2.69  -2.67  -2.62    261   1.01\n",
      "log_lik[32,3]  -3.29  6.0e-3   0.11  -3.54  -3.35  -3.28  -3.22  -3.11    320   1.01\n",
      "log_lik[33,3]  -2.96  6.2e-3   0.08  -3.13   -3.0  -2.95   -2.9  -2.84    153   1.02\n",
      "log_lik[34,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[35,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[36,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[37,3]  -5.15    0.02   0.26   -5.6  -5.35  -5.14  -4.97  -4.64    117   1.02\n",
      "log_lik[38,3]  -3.76  7.8e-3   0.15  -4.09  -3.83  -3.75  -3.65  -3.51    356   1.01\n",
      "log_lik[39,3]  -2.83  5.4e-3   0.06  -2.98  -2.87  -2.83  -2.79  -2.74    137   1.02\n",
      "log_lik[40,3]  -3.11  7.0e-3   0.09  -3.32  -3.16   -3.1  -3.04  -2.96    171   1.01\n",
      "log_lik[41,3]  -2.96  6.2e-3   0.08  -3.13   -3.0  -2.95   -2.9  -2.84    153   1.02\n",
      "log_lik[42,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[43,3]  -4.77    0.02   0.23  -5.17  -4.94  -4.77  -4.62  -4.32    119   1.02\n",
      "log_lik[44,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[45,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[46,3]  -2.74  4.7e-3   0.05  -2.86  -2.78  -2.74  -2.71  -2.66    124   1.02\n",
      "log_lik[47,3]  -2.83  5.4e-3   0.06  -2.98  -2.87  -2.83  -2.79  -2.74    137   1.02\n",
      "log_lik[48,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[49,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[50,3]  -3.29  6.0e-3   0.11  -3.54  -3.35  -3.28  -3.22  -3.11    320   1.01\n",
      "log_lik[51,3]   -2.7  2.7e-3   0.04  -2.79  -2.72  -2.69  -2.67  -2.62    261   1.01\n",
      "log_lik[52,3]  -3.29  6.0e-3   0.11  -3.54  -3.35  -3.28  -3.22  -3.11    320   1.01\n",
      "log_lik[53,3]  -3.76  7.8e-3   0.15  -4.09  -3.83  -3.75  -3.65  -3.51    356   1.01\n",
      "log_lik[54,3]  -2.83  5.4e-3   0.06  -2.98  -2.87  -2.83  -2.79  -2.74    137   1.02\n",
      "log_lik[55,3]  -4.69    0.01   0.22  -5.18  -4.81  -4.68  -4.53  -4.29    354   1.01\n",
      "log_lik[56,3]  -3.29  6.0e-3   0.11  -3.54  -3.35  -3.28  -3.22  -3.11    320   1.01\n",
      "log_lik[57,3]  -3.34  8.8e-3   0.11  -3.54  -3.42  -3.35  -3.27  -3.12    149   1.02\n",
      "log_lik[58,3]  -4.11    0.02   0.17  -4.42  -4.24   -4.1  -3.99  -3.76    126   1.02\n",
      "log_lik[59,3]  -3.57    0.01   0.13  -3.79  -3.67  -3.57  -3.48   -3.3    139   1.02\n",
      "log_lik[60,3]  -3.57    0.01   0.13  -3.79  -3.67  -3.57  -3.48   -3.3    139   1.02\n",
      "log_lik[61,3]  -2.83  5.4e-3   0.06  -2.98  -2.87  -2.83  -2.79  -2.74    137   1.02\n",
      "log_lik[62,3]  -2.76  2.8e-3   0.05  -2.87   -2.8  -2.76  -2.73  -2.66    330   1.01\n",
      "log_lik[63,3]  -2.66  3.1e-3   0.04  -2.74  -2.69  -2.66  -2.63   -2.6    158   1.01\n",
      "log_lik[64,3]  -3.57    0.01   0.13  -3.79  -3.67  -3.57  -3.48   -3.3    139   1.02\n",
      "log_lik[65,3]  -3.76  7.8e-3   0.15  -4.09  -3.83  -3.75  -3.65  -3.51    356   1.01\n",
      "log_lik[66,3]  -3.57    0.01   0.13  -3.79  -3.67  -3.57  -3.48   -3.3    139   1.02\n",
      "log_lik[0,4]   -5.37    0.02   0.26  -5.88  -5.55  -5.34   -5.2  -4.86    231   1.01\n",
      "log_lik[1,4]   -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[2,4]   -3.97  8.9e-3   0.15  -4.28  -4.08  -3.96  -3.86  -3.68    297   1.01\n",
      "log_lik[3,4]   -2.68  3.4e-3   0.04  -2.76   -2.7  -2.67  -2.65   -2.6    150   1.01\n",
      "log_lik[4,4]   -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[5,4]   -3.19  4.3e-3   0.09  -3.36  -3.25  -3.19  -3.14  -3.02    427   1.01\n",
      "log_lik[6,4]   -3.19  4.3e-3   0.09  -3.36  -3.25  -3.19  -3.14  -3.02    427   1.01\n",
      "log_lik[7,4]   -3.89  8.0e-3   0.15  -4.16  -3.98  -3.89  -3.79   -3.6    334   1.01\n",
      "log_lik[8,4]   -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[9,4]   -2.89  3.0e-3   0.06  -3.01  -2.93  -2.89  -2.84  -2.77    427   1.01\n",
      "log_lik[10,4]  -2.67  3.3e-3   0.04  -2.75  -2.69  -2.67  -2.64   -2.6    149   1.01\n",
      "log_lik[11,4]  -4.27    0.01   0.18  -4.62   -4.4  -4.26  -4.15  -3.94    277   1.01\n",
      "log_lik[12,4]  -2.81  3.7e-3   0.06  -2.93  -2.84   -2.8  -2.77  -2.71    235   1.01\n",
      "log_lik[13,4]  -3.25  4.8e-3    0.1  -3.46   -3.3  -3.24  -3.18  -3.09    402   1.01\n",
      "log_lik[14,4]  -2.92  4.0e-3   0.07  -3.07  -2.96  -2.92  -2.88  -2.81    287   1.01\n",
      "log_lik[15,4]  -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[16,4]  -3.19  4.3e-3   0.09  -3.36  -3.25  -3.19  -3.14  -3.02    427   1.01\n",
      "log_lik[17,4]  -3.19  4.3e-3   0.09  -3.36  -3.25  -3.19  -3.14  -3.02    427   1.01\n",
      "log_lik[18,4]  -5.25    0.02   0.25  -5.73  -5.43  -5.24  -5.07  -4.77    207   1.01\n",
      "log_lik[19,4]  -2.92  4.0e-3   0.07  -3.07  -2.96  -2.92  -2.88  -2.81    287   1.01\n",
      "log_lik[20,4]  -2.68  3.4e-3   0.04  -2.76   -2.7  -2.67  -2.65   -2.6    150   1.01\n",
      "log_lik[21,4]  -2.67  3.3e-3   0.04  -2.75  -2.69  -2.67  -2.64   -2.6    149   1.01\n",
      "log_lik[22,4]  -2.81  3.7e-3   0.06  -2.93  -2.84   -2.8  -2.77  -2.71    235   1.01\n",
      "log_lik[23,4]  -2.73  3.5e-3   0.05  -2.83  -2.76  -2.72  -2.69  -2.64    185   1.01\n",
      "log_lik[24,4]  -2.92  4.0e-3   0.07  -3.07  -2.96  -2.92  -2.88  -2.81    287   1.01\n",
      "log_lik[25,4]  -2.68  3.4e-3   0.04  -2.76   -2.7  -2.67  -2.65   -2.6    150   1.01\n",
      "log_lik[26,4]   -4.6    0.01    0.2  -5.01  -4.75  -4.59  -4.46  -4.22    259   1.01\n",
      "log_lik[27,4]   -3.7  7.0e-3   0.13  -3.97  -3.78  -3.69   -3.6  -3.46    360   1.01\n",
      "log_lik[28,4]  -3.07  4.0e-3   0.08  -3.25  -3.12  -3.06  -3.02  -2.93    393   1.01\n",
      "log_lik[29,4]  -2.67  3.3e-3   0.04  -2.75  -2.69  -2.67  -2.64   -2.6    149   1.01\n",
      "log_lik[30,4]  -3.62  6.5e-3   0.12  -3.86   -3.7  -3.63  -3.54  -3.37    368   1.01\n",
      "log_lik[31,4]  -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[32,4]  -2.89  3.0e-3   0.06  -3.01  -2.93  -2.89  -2.84  -2.77    427   1.01\n",
      "log_lik[33,4]  -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[34,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[35,4]  -4.27    0.01   0.18  -4.62   -4.4  -4.26  -4.15  -3.94    277   1.01\n",
      "log_lik[36,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[37,4]  -2.68  3.4e-3   0.04  -2.76   -2.7  -2.67  -2.65   -2.6    150   1.01\n",
      "log_lik[38,4]  -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[39,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[40,4]  -2.89  3.0e-3   0.06  -3.01  -2.93  -2.89  -2.84  -2.77    427   1.01\n",
      "log_lik[41,4]   -3.7  7.0e-3   0.13  -3.97  -3.78  -3.69   -3.6  -3.46    360   1.01\n",
      "log_lik[42,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[43,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[44,4]  -3.25  4.8e-3    0.1  -3.46   -3.3  -3.24  -3.18  -3.09    402   1.01\n",
      "log_lik[45,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[46,4]  -3.62  6.5e-3   0.12  -3.86   -3.7  -3.63  -3.54  -3.37    368   1.01\n",
      "log_lik[47,4]  -2.89  3.0e-3   0.06  -3.01  -2.93  -2.89  -2.84  -2.77    427   1.01\n",
      "log_lik[48,4]  -3.62  6.5e-3   0.12  -3.86   -3.7  -3.63  -3.54  -3.37    368   1.01\n",
      "log_lik[49,4]   -3.7  7.0e-3   0.13  -3.97  -3.78  -3.69   -3.6  -3.46    360   1.01\n",
      "log_lik[50,4]  -2.67  3.3e-3   0.04  -2.75  -2.69  -2.67  -2.64   -2.6    149   1.01\n",
      "log_lik[51,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[52,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[53,4]   -3.7  7.0e-3   0.13  -3.97  -3.78  -3.69   -3.6  -3.46    360   1.01\n",
      "log_lik[54,4]  -2.66  3.4e-3   0.04  -2.73  -2.68  -2.65  -2.63  -2.58    135   1.01\n",
      "log_lik[55,4]  -2.81  3.7e-3   0.06  -2.93  -2.84   -2.8  -2.77  -2.71    235   1.01\n",
      "log_lik[56,4]  -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[57,4]  -2.73  3.5e-3   0.05  -2.83  -2.76  -2.72  -2.69  -2.64    185   1.01\n",
      "log_lik[58,4]  -3.25  4.8e-3    0.1  -3.46   -3.3  -3.24  -3.18  -3.09    402   1.01\n",
      "log_lik[59,4]  -2.92  4.0e-3   0.07  -3.07  -2.96  -2.92  -2.88  -2.81    287   1.01\n",
      "log_lik[60,4]  -2.78  3.2e-3   0.05  -2.89  -2.82  -2.78  -2.74  -2.69    262   1.01\n",
      "log_lik[61,4]  -4.86    0.02   0.22  -5.29  -5.03  -4.86  -4.71  -4.43    219   1.01\n",
      "log_lik[62,4]  -3.39  4.7e-3   0.11   -3.6  -3.46  -3.39  -3.32  -3.18    499   1.01\n",
      "log_lik[63,4]  -3.62  6.5e-3   0.12  -3.86   -3.7  -3.63  -3.54  -3.37    368   1.01\n",
      "log_lik[64,4]  -2.71  3.2e-3   0.04   -2.8  -2.74  -2.71  -2.68  -2.63    198   1.01\n",
      "log_lik[65,4]  -2.81  3.7e-3   0.06  -2.93  -2.84   -2.8  -2.77  -2.71    235   1.01\n",
      "log_lik[66,4]  -3.46  5.7e-3   0.11   -3.7  -3.52  -3.45  -3.38  -3.26    386   1.01\n",
      "ypred[0]       31.84    0.09   5.64  20.63  28.14  31.86  35.56  42.94   3955    1.0\n",
      "ypred[1]       32.34    0.09   5.68  21.16  28.57  32.33  36.12  43.46   3990    1.0\n",
      "ypred[2]        31.9    0.09   5.72  20.57  27.98  32.01  35.82  43.01   4000    1.0\n",
      "ypred[3]       32.47     0.1   5.68  21.17   28.7  32.52  36.24   43.6   3536    1.0\n",
      "ypred[4]       32.19    0.09   5.72   20.9  28.41  32.19  36.08  43.19   3892    1.0\n",
      "mu_new         32.14    0.03   0.93  30.42  31.71  32.13  32.55  34.01    807    1.0\n",
      "ypred_new      32.21    0.09   5.64  21.14  28.38  32.19  36.02  43.15   3896    1.0\n",
      "lp__          -744.9    0.35   3.09 -751.6 -746.9 -744.7 -742.6 -739.6     80   1.03\n",
      "\n",
      "Samples were drawn using NUTS at Fri Dec  7 12:46:35 2018.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "print(hier_uni_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Attachment 4: Fit of hierarchical model with inverse gamma prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hier_inv_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "REMOVE EVERYTHING STARTING HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [13, 0, 13], [8, 0, 8], [9, 0, 9], [7, 0, 7], [10, 0, 10], [11, 0, 11], [11, 0, 11], [6, 0, 6], [3, 0, 3], [6, 0, 6], [30, 0, 30], [0, 0, 0], [8, 0, 8], [5, 0, 5], [14, 0, 14], [15, 0, 15], [16, 0, 16], [8, 0, 8], [4, 0, 4], [6, 0, 6], [0, 0, 0], [21, 0, 21]]\n",
      "23\n",
      "{'-11': [], '-10': [1.0, 1.0, 2.0, 2.0, 1.0, 6.0, 3.0, 1.0, 1.0, 1.0, 6.0, 5.0, 1.0], '-9': [6.0, 5.0, 1.0, 3.0, 1.0, 1.0, 1.0, 2.0], '-8': [3.0, 1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 4.0], '-7': [1.0, 1.0, 2.0, 3.0, 6.0, 4.0, 1.0], '-6': [1.0, 6.0, 2.0, 1.0, 1.0, 1.0, 5.0, 3.0, 4.0, 1.0], '-5': [2.0, 1.0, 1.0, 6.0, 2.0, 1.0, 4.0, 1.0, 1.0, 1.0, 6.0], '-4': [3.0, 1.0, 1.0, 6.0, 6.0, 6.0, 1.0, 1.0, 3.0, 4.0, 4.0], '-3': [4.0, 4.0, 5.0, 6.0, 3.0, 6.0], '-2': [1.0, 6.0, 5.0], '-1': [6.0, 6.0, 1.0, 1.0, 3.0, 5.0], '0': [6.0, 4.0, 6.0, 5.0, 6.0, 6.0, 6.0, 6.0, 5.0, 6.0, 6.0, 6.0, 6.0, 5.0, 6.0, 5.0, 6.0, 5.0, 6.0, 4.0, 6.0, 6.0, 5.0, 6.0, 4.0, 5.0, 6.0, 5.0, 6.0, 4.0], '1': [], '2': [4.0, 6.0, 4.0, 3.0, 5.0, 1.0, 2.0, 6.0], '3': [6.0, 4.0, 4.0, 6.0, 6.0], '4': [4.0, 6.0, 6.0, 3.0, 6.0, 6.0, 5.0, 1.0, 4.0, 6.0, 1.0, 1.0, 6.0, 4.0], '5': [1.0, 6.0, 4.0, 6.0, 6.0, 6.0, 6.0, 6.0, 1.0, 2.0, 6.0, 4.0, 3.0, 6.0, 1.0], '6': [6.0, 6.0, 6.0, 6.0, 1.0, 6.0, 6.0, 6.0, 6.0, 4.0, 6.0, 5.0, 6.0, 6.0, 6.0, 6.0], '7': [6.0, 1.0, 4.0, 3.0, 6.0, 5.0, 6.0, 1.0], '8': [6.0, 6.0, 6.0, 6.0], '9': [4.0, 6.0, 3.0, 6.0, 6.0, 6.0], '10': [], '11': [4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0, 2.0, 1.0, 1.0, 6.0, 4.0, 6.0, 3.0, 6.0, 6.0]}\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.loadtxt('data.txt')\n",
    "\n",
    "# Aggregate raw result\n",
    "aggregated_result = {}\n",
    "for i in range(-11,12):\n",
    "    aggregated_result[str(i)] = []\n",
    "\n",
    "for (spread, result) in raw_data:\n",
    "    key = str(int(spread))\n",
    "    aggregated_result[key].append(result)\n",
    "        \n",
    "# Turn to 23 x 3 matrix, where columns are win, lose, number of games, and rows are the possible spread\n",
    "data_matrix = []\n",
    "for key, value in aggregated_result.items():\n",
    "    total = len(value)\n",
    "    win = len([i for i in value if i > 0])\n",
    "    data_matrix.append([win, total - win, total])\n",
    "\n",
    "print(data_matrix)\n",
    "print(len(data_matrix))\n",
    "print(aggregated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read data and convert it to matrix with dimensions [6x23]\n",
    "raw_data = pd.read_csv('data.txt', sep=\" \", header=None)\n",
    "raw_data.columns = ['spread','win']\n",
    "raw_data = raw_data.groupby(['spread', 'win']).size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "data_json = {i: [0 for j in range(-11,12)] for i in range(1,7)}\n",
    "for index, row in raw_data.iterrows():\n",
    "    data_json[row['win']][11+row['spread']] = row['counts']\n",
    "\n",
    "data = pd.DataFrame(data=data_json)\n",
    "def getRowText():\n",
    "    j=-11\n",
    "    c = {}\n",
    "    for i in range(0,23):\n",
    "        c[i] = j\n",
    "        j += 1\n",
    "    return c\n",
    "\n",
    "data.rename(index=getRowText(), inplace=True)\n",
    "data = data.T\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertData(dataset):\n",
    "    win = dataset[0:3].sum(axis=0).values \n",
    "    lose = dataset[3:7].sum(axis=0).values \n",
    "    return np.array([win,lose])\n",
    "\n",
    "tt = convertData(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data #######\n",
    "raw_data = pd.read_csv('kristel.txt', sep=\" \", header=None)\n",
    "print(raw_data)\n",
    "\n",
    "\n",
    "highest_frequency = max([8,8,7,7,6,6,5,5,4,3,2])+2\n",
    "#data = dict(N=11, n=[highest_frequency for i in range(0,11)], \n",
    "#            y=np.array(list(reversed([8,8,7,7,6,6,5,5,4,3,2]))), x=[1,2,3,4,5,6,7,8,9,10,11])\n",
    "data = dict(r=23, c=3, y=data_matrix)\n",
    "\n",
    "def fit_model(model_code='stan_code\\\\binom-logistic-regression.stan'):\n",
    "    model = stan_utility.compile_model(model_code)\n",
    "    fit = model.sampling(data=data, seed=194838, chains=4, iter=4000)\n",
    "    samples = fit.extract(permuted=True)\n",
    "    print(fit)\n",
    "    stan_utility.check_treedepth(fit)\n",
    "    stan_utility.check_energy(fit)\n",
    "    stan_utility.check_div(fit)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "with open('stan_code\\\\binom-logistic-regression.stan') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# logistic_samples = fit_model('stan_code\\\\binom-logistic-regression.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_samples = fit_model('stan_code\\\\multinomial.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Multinomial #######\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_fit(x, y, mu):    \n",
    "    figsize = plt.rcParams['figure.figsize'].copy()\n",
    "    figsize[0] *= 2  # width\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # scatterplot and lines\n",
    "    color_scatter = 'C0'  # 'C0' for default color #0\n",
    "    color_line = 'C1'     # 'C1' for default color #1\n",
    "    # lighten color_line\n",
    "    color_shade = (1 - 0.1*(1 - np.array(mpl.colors.to_rgb(color_line))))\n",
    "    # plot\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        np.percentile(mu, 5, axis=0),\n",
    "        np.percentile(mu, 95, axis=0),\n",
    "        color=color_shade\n",
    "    )\n",
    "    ax.plot(\n",
    "        x,\n",
    "        np.percentile(mu, 50, axis=0),\n",
    "        color=color_line,\n",
    "        linewidth=1\n",
    "    )\n",
    "    ax.scatter(x, y, 5, color=color_scatter)\n",
    "    ax.set_xlabel('Spread')\n",
    "    ax.set_ylabel('Normalized frequency')\n",
    "    ax.set_title('Wins per spread')\n",
    "    plt.show()\n",
    "    \n",
    "show_model_fit(data[\"x\"],data[\"y\"]/np.array([highest_frequency]), logistic_samples['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"from scipy import stats\n",
    "for i in range(0,11):\n",
    "    mu = np.mean(logistic_samples['y_rep'][:,i])\n",
    "    std = np.std(logistic_samples['y_rep'][:,i])\n",
    "    x = np.linspace(0,11,100)\n",
    "    plt.plot(x, stats.norm(mu,std).pdf(x))\n",
    "    #plt.hist(logistic_samples['y_rep'][:,i], alpha=0.2, density=True)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plt.scatter(logistic_samples[\"alpha\"],logistic_samples[\"beta\"])\n",
    "plt.show();\n",
    "plt.hist(logistic_samples[\"alpha\"], bins=50)\n",
    "plt.hist(logistic_samples[\"beta\"], bins=50)\n",
    "plt.show();\n",
    "\n",
    "\"\"\"plt.scatter(data[\"x\"],data[\"y\"]/np.array([highest_frequency]))\n",
    "def calc(a,b,xp):\n",
    "    return np.exp(a + b*xp)/(1 + np.exp(a + b*xp))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR PSISLOO IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPPD: Posterior predictive distribution summarized by the simulation draws of theta^s\n",
    "def compute_LPPD(samples):\n",
    "    lppd_all = 0\n",
    "    samples = np.array(samples)\n",
    "    N = samples.shape[1]\n",
    "    S = samples.shape[0]\n",
    "    for i in range(0,N):\n",
    "        lppd_all += np.log(np.sum(np.exp(samples[:,i]))/S)\n",
    "    return lppd_all\n",
    "\n",
    "# LPPD LOO-CV: Bayesian loo-cv (leave-one-out cross-validation) estimate of out-of-sample predictive fit\n",
    "def compute_PSIS_LOO_values_and_plot_k(samples, model_text='Separate model'):\n",
    "    # Compute bayesian loo-cv using psisloo function\n",
    "    lppd_loo_cv, lppd_loos_cv, lppd_loo_k = psis.psisloo(samples)\n",
    "\n",
    "    #Estimate of the effective number of parameters\n",
    "    p_loo_cv = compute_LPPD(samples) - lppd_loo_cv\n",
    "    \n",
    "    # Plot k-values\n",
    "    plt.scatter(range(0,samples.shape[1]),lppd_loo_k,label=model_text, alpha=0.7)\n",
    "    \n",
    "    return {'loo_cv':lppd_loo_cv, 'loos_cv':lppd_loos_cv, 'loo_k':lppd_loo_k, 'p_loo_cv': p_loo_cv}\n",
    "\n",
    "#Reshape matrix of [SxNxJ] to [Sx(N*J)]\n",
    "def reshape_array(samples):\n",
    "    samples_all = np.array(samples)\n",
    "    S = samples_all.shape[0]\n",
    "    N = samples_all.shape[1]\n",
    "    J = samples_all.shape[2]\n",
    "    samples_reshaped = []\n",
    "    for s in range(0,S):\n",
    "        temp = []\n",
    "        for n in range(0,N):\n",
    "            for j in range(0,J):\n",
    "                temp.append(samples_all[s,n,j])\n",
    "        samples_reshaped.append(temp)\n",
    "    return np.array(samples_reshaped)\n",
    "\n",
    "\n",
    "#samples = sfit.extract(permuted=True)\n",
    "#compute_PSIS_LOO_values_and_plot_k(reshape_array(samples[\"log_lik\"]))\n",
    "\n",
    "# check the number of large (> 0.5) Pareto k estimates\n",
    "#np.sum(ks1 > 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
